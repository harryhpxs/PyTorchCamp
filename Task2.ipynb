{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task2\n",
    "\n",
    "Ref:\n",
    "1. https://pytorch.apachecn.org/docs/1.0/pytorch_with_examples.html\n",
    "2. https://towardsdatascience.com/linear-regression-with-pytorch-eb6dedead817"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy和PyTorch实现梯度下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 36123973.62207095\n",
      "1 38697817.65806739\n",
      "2 44781176.62733148\n",
      "3 44940620.18360953\n",
      "4 34195266.79405044\n",
      "5 18665911.449616555\n",
      "6 8183568.813241738\n",
      "7 3611387.8770753928\n",
      "8 1950145.3656844785\n",
      "9 1304456.7116623612\n",
      "10 993719.5546844036\n",
      "11 804274.9240340337\n",
      "12 669051.9732182666\n",
      "13 564419.5935986292\n",
      "14 480278.7578169184\n",
      "15 411335.1868939086\n",
      "16 354192.2099817307\n",
      "17 306380.5327330087\n",
      "18 266161.09821974655\n",
      "19 232143.21698095748\n",
      "20 203190.40776076156\n",
      "21 178468.01024074134\n",
      "22 157275.57403753654\n",
      "23 139029.657586293\n",
      "24 123263.42750879537\n",
      "25 109600.80486312733\n",
      "26 97710.71601418189\n",
      "27 87320.77806321466\n",
      "28 78216.83642167874\n",
      "29 70219.84410118137\n",
      "30 63185.06435718865\n",
      "31 57015.22838036384\n",
      "32 51545.26599525836\n",
      "33 46687.05932273003\n",
      "34 42364.9725352428\n",
      "35 38503.62959519438\n",
      "36 35051.55743916841\n",
      "37 31958.502378932288\n",
      "38 29182.126974432984\n",
      "39 26686.80675506353\n",
      "40 24452.494528913197\n",
      "41 22438.23073401255\n",
      "42 20616.42515322082\n",
      "43 18966.09356295305\n",
      "44 17470.19535286518\n",
      "45 16110.345678067803\n",
      "46 14872.466179897798\n",
      "47 13744.906506468462\n",
      "48 12715.102058801274\n",
      "49 11774.220308790293\n",
      "50 10913.315751704995\n",
      "51 10125.074935857261\n",
      "52 9401.8461756522\n",
      "53 8737.409583513629\n",
      "54 8126.617697836764\n",
      "55 7564.559919797437\n",
      "56 7047.205110272327\n",
      "57 6569.915410472593\n",
      "58 6129.805837651779\n",
      "59 5723.110195058671\n",
      "60 5346.900237254165\n",
      "61 4998.794928972979\n",
      "62 4676.948050717632\n",
      "63 4379.19958257279\n",
      "64 4103.071307793189\n",
      "65 3847.1242137622594\n",
      "66 3608.986041219357\n",
      "67 3387.647103753311\n",
      "68 3181.73992279466\n",
      "69 2990.033970531389\n",
      "70 2811.660478722097\n",
      "71 2645.3762147301795\n",
      "72 2490.3432613483315\n",
      "73 2345.509243088817\n",
      "74 2210.343815168706\n",
      "75 2083.9010022879747\n",
      "76 1965.675838474701\n",
      "77 1855.0233847844734\n",
      "78 1751.47583729807\n",
      "79 1654.5312291454584\n",
      "80 1563.585881770376\n",
      "81 1478.3286078033443\n",
      "82 1398.3601652223829\n",
      "83 1323.3274085826529\n",
      "84 1252.8166027455406\n",
      "85 1186.6072530004087\n",
      "86 1124.3351549872\n",
      "87 1065.7477931315066\n",
      "88 1010.6275752625149\n",
      "89 958.7360989327937\n",
      "90 909.8727090177117\n",
      "91 863.8397256507327\n",
      "92 820.5111546711921\n",
      "93 779.5765907306669\n",
      "94 740.9630135115201\n",
      "95 704.5204010015084\n",
      "96 670.1164985300228\n",
      "97 637.6162896884426\n",
      "98 606.911012126551\n",
      "99 577.8973967987253\n",
      "100 550.4371824948108\n",
      "101 524.4758465872313\n",
      "102 499.8771853443648\n",
      "103 476.5881764698805\n",
      "104 454.5270212666319\n",
      "105 433.6328260429344\n",
      "106 413.8239555738911\n",
      "107 395.0307321191588\n",
      "108 377.2030036042603\n",
      "109 360.2856207212453\n",
      "110 344.2335719554563\n",
      "111 328.9827884833197\n",
      "112 314.50101005549453\n",
      "113 300.7359087973422\n",
      "114 287.64684580376\n",
      "115 275.19876310236475\n",
      "116 263.36001439693064\n",
      "117 252.09218457881101\n",
      "118 241.36516279984755\n",
      "119 231.16460644402167\n",
      "120 221.44017115645863\n",
      "121 212.1703058242197\n",
      "122 203.33582612851578\n",
      "123 194.91455481662533\n",
      "124 186.884881152381\n",
      "125 179.22415546935162\n",
      "126 171.9197812800897\n",
      "127 164.942107478303\n",
      "128 158.28093463693725\n",
      "129 151.92256101069785\n",
      "130 145.8450281233616\n",
      "131 140.03838657331212\n",
      "132 134.48917055389464\n",
      "133 129.18640758572465\n",
      "134 124.11326555759265\n",
      "135 119.25964691598051\n",
      "136 114.61665485539469\n",
      "137 110.17323251679458\n",
      "138 105.9216145463759\n",
      "139 101.85138310563991\n",
      "140 97.95317332938403\n",
      "141 94.21860411355846\n",
      "142 90.63951068818585\n",
      "143 87.20954273327636\n",
      "144 83.92173933805176\n",
      "145 80.77041495320505\n",
      "146 77.74826389825526\n",
      "147 74.84974938307128\n",
      "148 72.0723276154886\n",
      "149 69.40465971205958\n",
      "150 66.84404004954047\n",
      "151 64.38642350941043\n",
      "152 62.02714699820505\n",
      "153 59.76362116184014\n",
      "154 57.58933458776049\n",
      "155 55.50098686352853\n",
      "156 53.49527333894947\n",
      "157 51.56699235972698\n",
      "158 49.71440281050887\n",
      "159 47.93314778256886\n",
      "160 46.220169286143495\n",
      "161 44.57277140417561\n",
      "162 42.988979723124054\n",
      "163 41.465058616296496\n",
      "164 39.99982986389164\n",
      "165 38.58926320354769\n",
      "166 37.23176271246023\n",
      "167 35.92530312381399\n",
      "168 34.66790105331142\n",
      "169 33.45777921305544\n",
      "170 32.29204588800962\n",
      "171 31.169575917248324\n",
      "172 30.089084958112792\n",
      "173 29.047921517769485\n",
      "174 28.04466145331193\n",
      "175 27.078032285532796\n",
      "176 26.146709456915\n",
      "177 25.24935152137428\n",
      "178 24.38436783352783\n",
      "179 23.55105469136612\n",
      "180 22.747588336502957\n",
      "181 21.973166295991675\n",
      "182 21.226096919554134\n",
      "183 20.505641049765607\n",
      "184 19.810822243453533\n",
      "185 19.140716007259368\n",
      "186 18.49438792657943\n",
      "187 17.870903829837182\n",
      "188 17.269360140100854\n",
      "189 16.6891923213856\n",
      "190 16.129707575765842\n",
      "191 15.589452656176366\n",
      "192 15.06790683321209\n",
      "193 14.564547230564365\n",
      "194 14.078678411692437\n",
      "195 13.609732801288983\n",
      "196 13.157001057274735\n",
      "197 12.71987950874147\n",
      "198 12.297904655687448\n",
      "199 11.89055615209596\n",
      "200 11.497098105211894\n",
      "201 11.11733745932732\n",
      "202 10.750297541089607\n",
      "203 10.39579416463027\n",
      "204 10.05338142544648\n",
      "205 9.722609049294725\n",
      "206 9.403049070365007\n",
      "207 9.094397719659025\n",
      "208 8.796341265005417\n",
      "209 8.508173215426028\n",
      "210 8.229743533724601\n",
      "211 7.960719173265312\n",
      "212 7.700834142787674\n",
      "213 7.449577908719956\n",
      "214 7.20670926698728\n",
      "215 6.97199206685119\n",
      "216 6.745124220811556\n",
      "217 6.5258954790668735\n",
      "218 6.313965218232922\n",
      "219 6.1090669174130845\n",
      "220 5.910976482085592\n",
      "221 5.719449331310016\n",
      "222 5.534279885543989\n",
      "223 5.355311037476756\n",
      "224 5.182241992593922\n",
      "225 5.01486644235479\n",
      "226 4.8530167772426\n",
      "227 4.6965531851807345\n",
      "228 4.545194423505732\n",
      "229 4.398820403432055\n",
      "230 4.257277383348578\n",
      "231 4.120349998817598\n",
      "232 3.9879100389858966\n",
      "233 3.859819217064089\n",
      "234 3.735939634821422\n",
      "235 3.616153692647022\n",
      "236 3.5002272685958706\n",
      "237 3.3881145596764894\n",
      "238 3.2796425821473667\n",
      "239 3.1747087128803684\n",
      "240 3.07317420387403\n",
      "241 2.9749463536155862\n",
      "242 2.8799042864100906\n",
      "243 2.7879436621331566\n",
      "244 2.69898216902423\n",
      "245 2.612908991167651\n",
      "246 2.529619692871783\n",
      "247 2.4490754111024575\n",
      "248 2.3711178011733844\n",
      "249 2.2956464055300474\n",
      "250 2.2226057952945455\n",
      "251 2.151927272204068\n",
      "252 2.083530141855232\n",
      "253 2.017336456362168\n",
      "254 1.9532785250438944\n",
      "255 1.8912874733392386\n",
      "256 1.8312843727746153\n",
      "257 1.7732200941797525\n",
      "258 1.7170353165712084\n",
      "259 1.662655012850582\n",
      "260 1.6099909386069455\n",
      "261 1.5590175464313702\n",
      "262 1.509683597231355\n",
      "263 1.4619292023775987\n",
      "264 1.4156985858408913\n",
      "265 1.3709506977126176\n",
      "266 1.3276421971572363\n",
      "267 1.2857043915261968\n",
      "268 1.2451218621033635\n",
      "269 1.2058306829981094\n",
      "270 1.1677809668427566\n",
      "271 1.130960505700238\n",
      "272 1.0953079761475022\n",
      "273 1.0607849816051929\n",
      "274 1.0273628793748306\n",
      "275 0.9950080779785071\n",
      "276 0.9636774774199002\n",
      "277 0.9333439430190048\n",
      "278 0.903974488864649\n",
      "279 0.875546302730063\n",
      "280 0.8480132977048711\n",
      "281 0.8213527255223687\n",
      "282 0.7955395728561363\n",
      "283 0.7705539432171482\n",
      "284 0.7463555835128277\n",
      "285 0.7229211029410193\n",
      "286 0.7002270643355439\n",
      "287 0.6782503024680993\n",
      "288 0.6569714309453337\n",
      "289 0.6363664185128927\n",
      "290 0.6164155507703165\n",
      "291 0.597090667670867\n",
      "292 0.5783825056911576\n",
      "293 0.5602597755527602\n",
      "294 0.5427089955843398\n",
      "295 0.5257137193004264\n",
      "296 0.5092600277110662\n",
      "297 0.4933189481056893\n",
      "298 0.47788120453556776\n",
      "299 0.4629305566491595\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# N是批大小；D_in是输入维度\n",
    "# H是隐藏层维度；D_out是输出维度  \n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 产生随机输入和输出数据\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# 随机初始化权重\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(300):\n",
    "    # 前向传播：计算预测值y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # 计算并显示loss（损失）\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # 反向传播，计算w1、w2对loss的梯度\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # 更新权重\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 25796850.0\n",
      "1 21595896.0\n",
      "2 21576594.0\n",
      "3 22803592.0\n",
      "4 22903994.0\n",
      "5 20504596.0\n",
      "6 15740988.0\n",
      "7 10547798.0\n",
      "8 6385464.0\n",
      "9 3743905.5\n",
      "10 2237164.0\n",
      "11 1423912.875\n",
      "12 978546.5\n",
      "13 724271.875\n",
      "14 568308.5625\n",
      "15 464823.5\n",
      "16 390766.0\n",
      "17 334398.6875\n",
      "18 289574.6875\n",
      "19 252845.6875\n",
      "20 222091.65625\n",
      "21 195962.21875\n",
      "22 173561.53125\n",
      "23 154213.96875\n",
      "24 137423.09375\n",
      "25 122762.921875\n",
      "26 109932.46875\n",
      "27 98670.5078125\n",
      "28 88753.0546875\n",
      "29 79986.8125\n",
      "30 72249.8828125\n",
      "31 65378.25\n",
      "32 59254.6953125\n",
      "33 53785.109375\n",
      "34 48892.20703125\n",
      "35 44503.8515625\n",
      "36 40562.28125\n",
      "37 37015.5546875\n",
      "38 33818.21875\n",
      "39 30933.30078125\n",
      "40 28324.7890625\n",
      "41 25963.8671875\n",
      "42 23823.33984375\n",
      "43 21879.82421875\n",
      "44 20113.16015625\n",
      "45 18505.390625\n",
      "46 17040.390625\n",
      "47 15704.0859375\n",
      "48 14483.88671875\n",
      "49 13369.140625\n",
      "50 12348.7177734375\n",
      "51 11413.833984375\n",
      "52 10557.171875\n",
      "53 9770.943359375\n",
      "54 9049.21484375\n",
      "55 8385.4619140625\n",
      "56 7774.97509765625\n",
      "57 7213.16064453125\n",
      "58 6695.6689453125\n",
      "59 6218.5849609375\n",
      "60 5778.07861328125\n",
      "61 5371.349609375\n",
      "62 4995.65283203125\n",
      "63 4648.45703125\n",
      "64 4327.42578125\n",
      "65 4030.65869140625\n",
      "66 3755.88134765625\n",
      "67 3501.173828125\n",
      "68 3265.110107421875\n",
      "69 3046.283935546875\n",
      "70 2843.16015625\n",
      "71 2654.5546875\n",
      "72 2479.45263671875\n",
      "73 2316.7392578125\n",
      "74 2165.48388671875\n",
      "75 2024.79150390625\n",
      "76 1893.8741455078125\n",
      "77 1772.2197265625\n",
      "78 1659.29638671875\n",
      "79 1554.041748046875\n",
      "80 1455.9346923828125\n",
      "81 1364.46240234375\n",
      "82 1279.107177734375\n",
      "83 1199.4012451171875\n",
      "84 1124.9921875\n",
      "85 1055.515869140625\n",
      "86 990.6135864257812\n",
      "87 929.9196166992188\n",
      "88 873.1800537109375\n",
      "89 820.1063232421875\n",
      "90 770.4439697265625\n",
      "91 723.95166015625\n",
      "92 680.437255859375\n",
      "93 639.6919555664062\n",
      "94 601.5228271484375\n",
      "95 565.77001953125\n",
      "96 532.2507934570312\n",
      "97 500.8201599121094\n",
      "98 471.347412109375\n",
      "99 443.6973876953125\n",
      "100 417.7527160644531\n",
      "101 393.3993835449219\n",
      "102 370.5419921875\n",
      "103 349.0819091796875\n",
      "104 328.9289245605469\n",
      "105 310.0135192871094\n",
      "106 292.25469970703125\n",
      "107 275.5572509765625\n",
      "108 259.8619079589844\n",
      "109 245.10208129882812\n",
      "110 231.21905517578125\n",
      "111 218.16571044921875\n",
      "112 205.8782501220703\n",
      "113 194.3131103515625\n",
      "114 183.43096923828125\n",
      "115 173.18385314941406\n",
      "116 163.53514099121094\n",
      "117 154.45025634765625\n",
      "118 145.891845703125\n",
      "119 137.8274688720703\n",
      "120 130.2283935546875\n",
      "121 123.06619262695312\n",
      "122 116.31353759765625\n",
      "123 109.94670104980469\n",
      "124 103.94316101074219\n",
      "125 98.28067016601562\n",
      "126 92.94232940673828\n",
      "127 87.90438842773438\n",
      "128 83.14836883544922\n",
      "129 78.66114807128906\n",
      "130 74.42427062988281\n",
      "131 70.42604064941406\n",
      "132 66.64839172363281\n",
      "133 63.082759857177734\n",
      "134 59.71455383300781\n",
      "135 56.53290557861328\n",
      "136 53.52796936035156\n",
      "137 50.687889099121094\n",
      "138 48.00374221801758\n",
      "139 45.46607208251953\n",
      "140 43.068294525146484\n",
      "141 40.80241394042969\n",
      "142 38.65799331665039\n",
      "143 36.63187789916992\n",
      "144 34.71440124511719\n",
      "145 32.900978088378906\n",
      "146 31.18546485900879\n",
      "147 29.561763763427734\n",
      "148 28.025970458984375\n",
      "149 26.572162628173828\n",
      "150 25.195716857910156\n",
      "151 23.89352798461914\n",
      "152 22.660812377929688\n",
      "153 21.4932861328125\n",
      "154 20.388063430786133\n",
      "155 19.340822219848633\n",
      "156 18.349693298339844\n",
      "157 17.41036605834961\n",
      "158 16.520614624023438\n",
      "159 15.679390907287598\n",
      "160 14.88231086730957\n",
      "161 14.126752853393555\n",
      "162 13.410560607910156\n",
      "163 12.731931686401367\n",
      "164 12.08863353729248\n",
      "165 11.478372573852539\n",
      "166 10.899765014648438\n",
      "167 10.35173225402832\n",
      "168 9.831565856933594\n",
      "169 9.338271141052246\n",
      "170 8.870685577392578\n",
      "171 8.426809310913086\n",
      "172 8.005615234375\n",
      "173 7.60627555847168\n",
      "174 7.22714900970459\n",
      "175 6.8673295974731445\n",
      "176 6.5261688232421875\n",
      "177 6.202175140380859\n",
      "178 5.8944549560546875\n",
      "179 5.602787494659424\n",
      "180 5.325664043426514\n",
      "181 5.062488555908203\n",
      "182 4.812726974487305\n",
      "183 4.575377464294434\n",
      "184 4.350192070007324\n",
      "185 4.136409759521484\n",
      "186 3.9330644607543945\n",
      "187 3.740098476409912\n",
      "188 3.5567121505737305\n",
      "189 3.382805824279785\n",
      "190 3.2171783447265625\n",
      "191 3.0599477291107178\n",
      "192 2.910677433013916\n",
      "193 2.7687790393829346\n",
      "194 2.633888006210327\n",
      "195 2.5057504177093506\n",
      "196 2.3837482929229736\n",
      "197 2.2680811882019043\n",
      "198 2.1581077575683594\n",
      "199 2.053485155105591\n",
      "200 1.9540622234344482\n",
      "201 1.8594342470169067\n",
      "202 1.769568920135498\n",
      "203 1.6839778423309326\n",
      "204 1.6026952266693115\n",
      "205 1.5254446268081665\n",
      "206 1.4519530534744263\n",
      "207 1.3819704055786133\n",
      "208 1.3155481815338135\n",
      "209 1.252251386642456\n",
      "210 1.1922391653060913\n",
      "211 1.1349372863769531\n",
      "212 1.0805389881134033\n",
      "213 1.0287386178970337\n",
      "214 0.9795747399330139\n",
      "215 0.932640552520752\n",
      "216 0.8880584239959717\n",
      "217 0.8457037210464478\n",
      "218 0.8052813410758972\n",
      "219 0.766860842704773\n",
      "220 0.7302777171134949\n",
      "221 0.6954779624938965\n",
      "222 0.6624265313148499\n",
      "223 0.6308993101119995\n",
      "224 0.6009199023246765\n",
      "225 0.5723374485969543\n",
      "226 0.545188844203949\n",
      "227 0.5192991495132446\n",
      "228 0.49467554688453674\n",
      "229 0.4712834358215332\n",
      "230 0.44894999265670776\n",
      "231 0.4277021884918213\n",
      "232 0.40741997957229614\n",
      "233 0.38820680975914\n",
      "234 0.36983510851860046\n",
      "235 0.35244470834732056\n",
      "236 0.3357779383659363\n",
      "237 0.3199562430381775\n",
      "238 0.30488401651382446\n",
      "239 0.2905273139476776\n",
      "240 0.27682727575302124\n",
      "241 0.26375946402549744\n",
      "242 0.251384973526001\n",
      "243 0.23958003520965576\n",
      "244 0.22835394740104675\n",
      "245 0.21759043633937836\n",
      "246 0.20737244188785553\n",
      "247 0.19764140248298645\n",
      "248 0.18835687637329102\n",
      "249 0.179551899433136\n",
      "250 0.17112553119659424\n",
      "251 0.16311301290988922\n",
      "252 0.15547454357147217\n",
      "253 0.14821916818618774\n",
      "254 0.14127793908119202\n",
      "255 0.13469800353050232\n",
      "256 0.12842872738838196\n",
      "257 0.12239393591880798\n",
      "258 0.11668232083320618\n",
      "259 0.1112256795167923\n",
      "260 0.10604161024093628\n",
      "261 0.10108411312103271\n",
      "262 0.09637995064258575\n",
      "263 0.09191489219665527\n",
      "264 0.08763134479522705\n",
      "265 0.08355394005775452\n",
      "266 0.07963302731513977\n",
      "267 0.07595914602279663\n",
      "268 0.07242365181446075\n",
      "269 0.06905216723680496\n",
      "270 0.06585243344306946\n",
      "271 0.06278738379478455\n",
      "272 0.05988591909408569\n",
      "273 0.057109542191028595\n",
      "274 0.05446215346455574\n",
      "275 0.05194371938705444\n",
      "276 0.04954327642917633\n",
      "277 0.04725049436092377\n",
      "278 0.04505881667137146\n",
      "279 0.042981214821338654\n",
      "280 0.040989167988300323\n",
      "281 0.03910912573337555\n",
      "282 0.03730279952287674\n",
      "283 0.03558017313480377\n",
      "284 0.033947743475437164\n",
      "285 0.032386619597673416\n",
      "286 0.030897047370672226\n",
      "287 0.0294732004404068\n",
      "288 0.02811449207365513\n",
      "289 0.026812445372343063\n",
      "290 0.02558865025639534\n",
      "291 0.024416670203208923\n",
      "292 0.02329828031361103\n",
      "293 0.022232558578252792\n",
      "294 0.021207643672823906\n",
      "295 0.020247317850589752\n",
      "296 0.01931646093726158\n",
      "297 0.018436100333929062\n",
      "298 0.017595011740922928\n",
      "299 0.01679266057908535\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "\n",
    "# N是批大小； D_in 是输入维度；\n",
    "# H 是隐藏层维度； D_out 是输出维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 产生随机输入和输出数据\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# 随机初始化权重\n",
    "w1 = torch.randn(D_in, H, device=device)\n",
    "w2 = torch.randn(H, D_out, device=device)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(300):\n",
    "    # 前向传播：计算预测值y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # 计算并输出loss；loss是存储在PyTorch的tensor中的标量，维度是()（零维标量）；我们使用loss.item()得到tensor中的纯python数值。\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # 反向传播，计算w1、w2对loss的梯度\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # 使用梯度下降更新权重\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy和PyTorch实现线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2333)\n",
    "\n",
    "# create dummy data for training\n",
    "x_values = [i for i in range(11)]\n",
    "x_train = np.array(x_values, dtype=np.float32)\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "\n",
    "y_values = [2*i + 1 + np.random.random() for i in x_values]\n",
    "y_train = np.array(y_values, dtype=np.float32)\n",
    "y_train = y_train.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.5297776]\n",
      " [ 3.8944874]\n",
      " [ 5.242338 ]\n",
      " [ 7.9269605]\n",
      " [ 9.191861 ]\n",
      " [11.387585 ]\n",
      " [13.287302 ]\n",
      " [15.729676 ]\n",
      " [17.43938  ]\n",
      " [19.30325  ]\n",
      " [21.393335 ]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class linearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(linearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "    \n",
    "inputDim = 1        # takes variable 'x' \n",
    "outputDim = 1       # takes variable 'y'\n",
    "learningRate = 0.01 \n",
    "epochs = 100\n",
    "\n",
    "model = linearRegression(inputDim, outputDim)\n",
    "##### For GPU #######\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    \n",
    "criterion = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(304.3204, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 0, loss 304.3204040527344\n",
      "tensor(25.2250, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 1, loss 25.225019454956055\n",
      "tensor(2.4562, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 2, loss 2.4561500549316406\n",
      "tensor(0.5951, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 3, loss 0.5950658321380615\n",
      "tensor(0.4394, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 4, loss 0.43940770626068115\n",
      "tensor(0.4229, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 5, loss 0.42289915680885315\n",
      "tensor(0.4178, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 6, loss 0.41778305172920227\n",
      "tensor(0.4136, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 7, loss 0.41363832354545593\n",
      "tensor(0.4096, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 8, loss 0.40961435437202454\n",
      "tensor(0.4056, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 9, loss 0.4056415557861328\n",
      "tensor(0.4017, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 10, loss 0.40171337127685547\n",
      "tensor(0.3978, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 11, loss 0.3978293240070343\n",
      "tensor(0.3940, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 12, loss 0.39398863911628723\n",
      "tensor(0.3902, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 13, loss 0.39019060134887695\n",
      "tensor(0.3864, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 14, loss 0.386435329914093\n",
      "tensor(0.3827, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 15, loss 0.382721871137619\n",
      "tensor(0.3790, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 16, loss 0.37904980778694153\n",
      "tensor(0.3754, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 17, loss 0.3754187822341919\n",
      "tensor(0.3718, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 18, loss 0.37182819843292236\n",
      "tensor(0.3683, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 19, loss 0.3682779371738434\n",
      "tensor(0.3648, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 20, loss 0.3647671043872833\n",
      "tensor(0.3613, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 21, loss 0.36129558086395264\n",
      "tensor(0.3579, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 22, loss 0.3578629195690155\n",
      "tensor(0.3545, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 23, loss 0.35446837544441223\n",
      "tensor(0.3511, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 24, loss 0.35111185908317566\n",
      "tensor(0.3478, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 25, loss 0.3477928340435028\n",
      "tensor(0.3445, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 26, loss 0.34451085329055786\n",
      "tensor(0.3413, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 27, loss 0.34126555919647217\n",
      "tensor(0.3381, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 28, loss 0.33805641531944275\n",
      "tensor(0.3349, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 29, loss 0.3348832130432129\n",
      "tensor(0.3317, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 30, loss 0.331745445728302\n",
      "tensor(0.3286, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 31, loss 0.3286426365375519\n",
      "tensor(0.3256, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 32, loss 0.32557448744773865\n",
      "tensor(0.3225, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 33, loss 0.32254067063331604\n",
      "tensor(0.3195, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 34, loss 0.31954070925712585\n",
      "tensor(0.3166, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 35, loss 0.3165741562843323\n",
      "tensor(0.3136, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 36, loss 0.3136405646800995\n",
      "tensor(0.3107, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 37, loss 0.3107401132583618\n",
      "tensor(0.3079, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 38, loss 0.30787190794944763\n",
      "tensor(0.3050, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 39, loss 0.30503562092781067\n",
      "tensor(0.3022, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 40, loss 0.30223119258880615\n",
      "tensor(0.2995, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 41, loss 0.2994578778743744\n",
      "tensor(0.2967, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 42, loss 0.29671570658683777\n",
      "tensor(0.2940, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 43, loss 0.29400408267974854\n",
      "tensor(0.2913, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 44, loss 0.2913227379322052\n",
      "tensor(0.2887, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 45, loss 0.2886713445186615\n",
      "tensor(0.2860, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 46, loss 0.28604966402053833\n",
      "tensor(0.2835, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 47, loss 0.2834570109844208\n",
      "tensor(0.2809, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 48, loss 0.2808935344219208\n",
      "tensor(0.2784, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 49, loss 0.2783585786819458\n",
      "tensor(0.2759, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 50, loss 0.27585190534591675\n",
      "tensor(0.2734, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 51, loss 0.2733733057975769\n",
      "tensor(0.2709, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 52, loss 0.27092236280441284\n",
      "tensor(0.2685, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 53, loss 0.2684987485408783\n",
      "tensor(0.2661, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 54, loss 0.2661021649837494\n",
      "tensor(0.2637, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 55, loss 0.2637324929237366\n",
      "tensor(0.2614, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 56, loss 0.2613891661167145\n",
      "tensor(0.2591, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 57, loss 0.25907209515571594\n",
      "tensor(0.2568, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 58, loss 0.25678086280822754\n",
      "tensor(0.2545, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 59, loss 0.2545151114463806\n",
      "tensor(0.2523, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 60, loss 0.25227487087249756\n",
      "tensor(0.2501, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 61, loss 0.2500593364238739\n",
      "tensor(0.2479, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 62, loss 0.24786873161792755\n",
      "tensor(0.2457, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 63, loss 0.2457025945186615\n",
      "tensor(0.2436, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 64, loss 0.24356068670749664\n",
      "tensor(0.2414, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 65, loss 0.24144266545772552\n",
      "tensor(0.2393, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 66, loss 0.23934826254844666\n",
      "tensor(0.2373, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 67, loss 0.23727728426456451\n",
      "tensor(0.2352, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 68, loss 0.23522943258285522\n",
      "tensor(0.2332, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 69, loss 0.23320436477661133\n",
      "tensor(0.2312, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 70, loss 0.2312019020318985\n",
      "tensor(0.2292, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 71, loss 0.229221910238266\n",
      "tensor(0.2273, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 72, loss 0.22726397216320038\n",
      "tensor(0.2253, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 73, loss 0.22532792389392853\n",
      "tensor(0.2234, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 74, loss 0.22341345250606537\n",
      "tensor(0.2215, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 75, loss 0.22152024507522583\n",
      "tensor(0.2196, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 76, loss 0.21964842081069946\n",
      "tensor(0.2178, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 77, loss 0.21779736876487732\n",
      "tensor(0.2160, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 78, loss 0.21596702933311462\n",
      "tensor(0.2142, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 79, loss 0.21415705978870392\n",
      "tensor(0.2124, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 80, loss 0.21236738562583923\n",
      "tensor(0.2106, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 81, loss 0.21059773862361908\n",
      "tensor(0.2088, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 82, loss 0.20884771645069122\n",
      "tensor(0.2071, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 83, loss 0.2071174532175064\n",
      "tensor(0.2054, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 84, loss 0.2054062932729721\n",
      "tensor(0.2037, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 85, loss 0.20371432602405548\n",
      "tensor(0.2020, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 86, loss 0.20204132795333862\n",
      "tensor(0.2004, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 87, loss 0.20038677752017975\n",
      "tensor(0.1988, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 88, loss 0.19875091314315796\n",
      "tensor(0.1971, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 89, loss 0.19713331758975983\n",
      "tensor(0.1955, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 90, loss 0.19553370773792267\n",
      "tensor(0.1940, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 91, loss 0.19395194947719574\n",
      "tensor(0.1924, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 92, loss 0.19238781929016113\n",
      "tensor(0.1908, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 93, loss 0.1908412128686905\n",
      "tensor(0.1893, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 94, loss 0.18931184709072113\n",
      "tensor(0.1878, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 95, loss 0.18779969215393066\n",
      "tensor(0.1863, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 96, loss 0.18630437552928925\n",
      "tensor(0.1848, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 97, loss 0.18482564389705658\n",
      "tensor(0.1834, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 98, loss 0.1833634376525879\n",
      "tensor(0.1819, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 99, loss 0.18191766738891602\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # Converting inputs and labels to Variable\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = Variable(torch.from_numpy(x_train).cuda())\n",
    "        labels = Variable(torch.from_numpy(y_train).cuda())\n",
    "    else:\n",
    "        inputs = Variable(torch.from_numpy(x_train))\n",
    "        labels = Variable(torch.from_numpy(y_train))\n",
    "\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = criterion(outputs, labels)\n",
    "    print(loss)\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.9438095]\n",
      " [ 3.0146153]\n",
      " [ 5.085421 ]\n",
      " [ 7.156227 ]\n",
      " [ 9.227033 ]\n",
      " [11.297838 ]\n",
      " [13.368645 ]\n",
      " [15.43945  ]\n",
      " [17.510256 ]\n",
      " [19.581062 ]\n",
      " [21.651867 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3WlwVOed7/Hvo7W1tlpCaKHVksAYIYQQWHYg4C3gjJM4TqKYcTLlLA4Z34nH9kzuEI/vfZNUZqri1BB7citOUr4TL7lxnJtJ5Ilr7sQ2GC+xDcZgYwWD2CUhQHtr75bU0nNfSMiAEQihVm+/T5Wq1adP9/l3I/149Jxz/sdYaxERkcgXF+oCRERkdijQRUSihAJdRCRKKNBFRKKEAl1EJEoo0EVEooQCXUQkSijQRUSihAJdRCRKJMzlxubNm2dLSkrmcpMiIhFvz549Hdba3EutN6eBXlJSwu7du+dykyIiEc8Y0zid9TTlIiISJRToIiJRQoEuIhIl5nQO/UJGRkZobm7G7/eHupSo5nA4cLvdJCYmhroUEQmSkAd6c3MzGRkZlJSUYIwJdTlRyVpLZ2cnzc3NlJaWhrocEQmSkE+5+P1+cnJyFOZBZIwhJydHfwWJRLmQBzqgMJ8D+oxFol9YBLqISLTyDY8yMBSYk22FfA491Do7O1m/fj0ALS0txMfHk5s7fkLWrl27SEpKCsp2161bx09+8hOqqqqmXOeRRx7h3nvvxeFwBKUGEQkeay2H2/r59Tvvc6zvXRJS9+Jxeqgpq6EyvzIo24y4QK9rqaO2vpamnqZZ+XBycnLYu3cvAN/73vdIT09n8+bN56xjrcVaS1zc3P5B88gjj/CNb3xDgS4SYfqHAmyvb+OtY0fZ0/YyC/MHyMt04/V52bJjC5vXbA5KqEfUlEtdSx1bdmzB6/PiPuvDqWupm/VtHTlyhIqKCv7mb/6GVatWceLECbKysiYf/81vfsM3v/lNAFpbW6mpqaG6uprrrruOnTt3fuT1BgcH2bhxI5WVlXzpS186ZwflPffcQ3V1NcuWLeP73/8+AI8++ihtbW1cf/31bNiwYcr1RCS8DAVG+dXORho7BujlbZYWeSlwphFn4nCluHA5XNTW1wZl2xE1Qq+tr8XlcOFKcQFM3tbW1wblf7v9+/fz5JNP8vOf/5xAYOo5sAceeIAHH3yQ1atX09DQwG233ca+ffvOWecnP/kJLpeLuro63nvvPaqrqycfe/jhh8nOziYQCHDzzTdzxx138O1vf5sf/ehH/OlPf5r8j+RC65WXl8/6+xaRy+cbHiUlKZ7khHiuXzyPQmcK/7BtP+4U9znrOR1OmnqaglJDRAV6U08T7sy5+3AWLVrEtddee8n1tm3bxsGDByfve71efD4fKSkpk8tef/11HnzwQQBWrlzJsmXLJh979tln+cUvfkEgEODUqVPs37//gkE93fVEZO6MjVn2Nnfz1pEOPruikOKcNJYVOgHwOD14fd7JwSdAj78Hj9MTlFoiKtDn+sNJS0ub/D4uLg5r7eT9s6dMrLXT2oF6oUMHDx8+zI9//GN27dpFVlYWd9111wWPF5/ueiIydzr7h9i6v5XTPX4W5qaRnXZuBtSU1bBlxxZgfPDZ4+/B6/eyaeWmoNQTUXPoNWU1eP1evD4vY3YMr8+L1++lpqwm6NuOi4vD5XJx+PBhxsbGeO655yYf27BhA4899tjk/TM7Wc92ww038MwzzwDw/vvv88EHHwDQ29tLRkYGmZmZnD59mhdffHHyORkZGfT19V1yPRGZe3savTzzdhPdvhE+tTyf21cUkuE4t7VGZX4lm9dsxpXiorm3GVeKK2g7RCHCRuhnPpyzj3LZtHJT0D6c8/3whz/k1ltvxePxUF5eztDQEACPPfYY3/rWt3jyyScn57fPDniA++67j6997WtUVlayatWqyTn0VatWUV5eTkVFBQsXLmTt2rWTz7nnnnvYsGEDRUVFbN26dcr1RGTuJcYbrpqfzk1LcklNmjpKK/Mr5yyjzNnTCMFWXV1tz7/AxYEDB1i6dOmc1RDL9FmLzNzI6Bg7j3WSnZbEskIn1to5OwPbGLPHWlt9qfUiaoQuIhIKJ7oG2Xagle7BEa4pHt+HF47tNBToIiJT8I+M8uaRDuqae8hKTeSOa9wUZaeGuqwpKdBFRKbQ0uPnzyd7uKbYxZpFOSTGh/dxJJeszhhTZIx5xRhzwBjzgTHm7yaWZxtjthpjDk/cui71WiIi4W5wOMCRtvGjy0rmpXH3x0u54ercsA9zmN5hiwHgH6y1S4HVwN8aY8qBh4CXrbWLgZcn7ouIRCRrLQdb+vjljkZe/KAV3/AoAM7UyLnK1yWnXKy1p4HTE9/3GWMOAAuAzwE3Taz2NPAq8I9BqVJEJIj6/CNsr2/jWPsA+U4Ht5TnkZIUH+qyLttl/Q1hjCkBVgJvA3kTYX8m9OfPdnFzJT4+nqqqKioqKti4cSODg4Mzfq1XX32V2267DYDnn3+ehx9+eMp1u7u7+elPfzp5/9SpU9xxxx0z3raIXL6hwCjPvN3Eia5Bbrg6lzuri5iXnhzqsmZk2oFujEkHfg/8vbW29zKed48xZrcxZnd7e/tMagy6lJQU9u7dy759+0hKSuLnP//5OY9baxkbG7vs17399tt56KGpZ6LOD/TCwkJ+97vfXfZ2ROTyDQ6PN9w700zrrtXFXFPsIi4u/A5HnK5pBboxJpHxMH/GWnum72OrMaZg4vECoO1Cz7XWPm6trbbWVp+5cEQ4u/766zly5AgNDQ0sXbqUe++9d7J97ksvvcSaNWtYtWoVGzdupL+/H4AXXniBsrIy1q1bR23th20xn3rqKe677z5gvMXuF77wBVasWMGKFSt46623eOihhzh69ChVVVV85zvfoaGhgYqKCmC8V8zdd9/N8uXLWblyJa+88srka9bU1HDrrbeyePHiyYZfo6OjfP3rX6eiooLly5fz6KOPzuXHJhIxxsYsexq9PPHGcRo6BgBYVugkKzU4F7OZS5ecQzfjR8//AjhgrX3krIeeB74GPDxx+4fZKOjfd5/4yLKr8zJYUZTFyOgY//HeyY88Xl6YybJCJ77hUf6z7tQ5j22sLpr2tgOBAH/84x+59dZbATh48CBPPvkkP/3pT+no6OCf//mf2bZtG2lpafzwhz/kkUce4cEHH+Sv//qv2b59O1dddRV33nnnBV/7gQce4MYbb+S5555jdHSU/v5+Hn74Yfbt2zfZ+6WhoWFy/TOtA/785z9TX1/PJz/5SQ4dOgSM94p57733SE5OZsmSJdx///20tbVx8uTJyba93d3d037fIrGiY6KZVstEM62c9MgP8bNNZ4S+FvgK8AljzN6Jr08zHuS3GGMOA7dM3I9IPp+Pqqoqqqur8Xg8bNo03gmtuLiY1atXA7Bz507279/P2rVrqaqq4umnn6axsZH6+npKS0tZvHgxxhjuuuuuC25j+/btfOtb3wLG5+ydTudFa3rjjTf4yle+AkBZWRnFxcWTgb5+/XqcTicOh4Py8nIaGxtZuHAhx44d4/777+eFF14gMzNzVj4bkWixp7GLX7/dRI9vhE8vL7hgM61IN52jXN4ApppUWj+75Vx8RJ0YH3fRx1OS4i9rRD75vIk59POd3T7XWsstt9zCs88+e846e/fuDcopwBfrsZOc/OEOm/j4eAKBAC6Xi/fff58XX3yRxx57jN/+9rc88cQTs16XSCSqa6njyb0v0NDZS1VxEkN8HmPmpmHWXAr/I+XDxOrVq3nzzTc5cuQIMH5JuUOHDlFWVsbx48c5evQowEcC/4z169fzs5/9DBif7z7TDvdMe9zznd1u99ChQzQ1NbFkyZIp6+vo6GBsbIwvfvGL/NM//RPvvvvujN+rSDQYDozx2qF2nqvbzZYdW4hLPMmq0jj6hjuDdunKUFOgT1Nubi5PPfUUX/7yl6msrGT16tXU19fjcDh4/PHH+cxnPsO6desoLi6+4PN//OMf88orr7B8+XKuueYaPvjgA3Jycli7di0VFRV85zvfOWf9e++9l9HRUZYvX86dd97JU089dc7I/HwnT57kpptuoqqqiq9//ev84Ac/mNX3LxJJTnQN8qudjbzb6OWPh17H5XCRneqak+t6hpLa58YQfdYS7fwjo/zpcAf7To4309qwNI/v/uk+3Jlu4syH49cxO0ZzbzNPfC4ypiXVPldEYk5Lj5/9p3qpLnGxeuF4M625vnRlKGnKRUQi2uBwgMOtHzbT+vrHS7h+8YfNtEJ56cq5FhaBPpfTPrFKn7FEG2stB0738ssdjby0f+pmWnN9Xc9QCvmUi8PhoLOzk5ycnLC8Akg0sNbS2dmJw+EIdSkis6LXP8L2A20c7xigYBrNtObyup6hFPJAd7vdNDc3E659XqKFw+HA7XaHugyRKzYUGOWZnU2Mjo1x45JcqtxZEd1/ZTaFPNATExMpLS0NdRkiEuYGhgKkJSeQnBDPjVfnsiArJaJ6lc+FsJhDFxGZytiYZXdD1znNtMoLMxXmFxDyEbqIyFTa+vxs299Ga6+fq+anMy8jMvuUzxUFuoiEpXcaunjrSCeOxDhuqyzgqvnpOnDiEhToIhKWHAnxLMnP4MarcyPycnChoEAXkbAwHBjjraMdzEtPpmKBk+Xu8S+ZPgW6iIRcU+cgWw+00usb4dqS7FCXE7EU6CISMv6RUV4/1M4Hp3pxpSaysdqN25Ua6rIilgJdROZMXUsdtfW1NPU04XF6WJN/O/WnM7i2JJuPLcye7L8iM6NPT0TmRF1LHVt2bKG9v5f0uMV4fV6eOfCvXHvVAOsWz1OYzwJ9giIyJ35/oJa4QBEtbVfT3JZNRlI2LoeLlxr+I9SlRQ1NuYhI0PX4RnjnqCXBekhPGcEzv5uEeIvT4aSppynU5UUNBbqIBNVQYJRfv92EwxSSntFM6fx4zpwfFK0XmggVTbmISFAMDAUASE6I56YluWxe/zFIaqTbH/0XmggVjdBFZFaNjlnebfKy82gnt60opHReGksLMoEq0h2bzznKZdPKTTHRp3yuKNBFZNa09frZeqCVtt4hFuelM/+8ZlqxcqGJUFGgi8is2HW8ix1HO0lJGm+mtTgvI9QlxRwFuojMitSkeMoKxptpORLVTCsUFOgiMiPDgTHePDLeTGu520nFgvEvCR0FuohctoaOAbYdaKV/KKBmWmFEgS4i0+YfGeXVg+0cON1LdloSf1ldRGFWSqjLkgkKdBGZttZePwdb+vhYaTbXlWaToP4rYUWBLiIXNTAUoNnrY0l+BsU5ady9roRMhy7QHI4U6CJyQdZa9p/u5bVD7VgLxTmpOBLjFeZhTIEuIh/R4xvh5QOtNHYOssCVwi1L83QoYgRQoIvEmPMvMlFTVnPO2ZtnmmmNWcsnyuZT6XZiznTTkrCmPRoiMeTMRSa8Pi/uTDden5ctO7ZQ11JH/1nNtG4uy+Ura4pZUZSlMI8gGqGLxJDa+lpcDheuFBcArhQX1sJP33qJcmcKn51oplWWnxniSmUmNEIXiSFNPU04HR+ezTnoT6S1fTH1pyyLctPJy0y+yLMl3GmELhJDPE4PXp8XV4qLlq50WroyGRnrp6pklM9UFoS6PLlCGqGLxJCashq8fi9en5f4+ABJyW24cur4RvWnQ12azIJLBrox5gljTJsxZt9Zy75njDlpjNk78aWfBpEwNxQYpaM7j88vfABXigsfhygvCvDg2v+uHuVRYjpTLk8BPwF+ed7yR621W2a9IhGZdcc7Bnh5opnWdSUl1Kz4XqhLkiC4ZKBba183xpQEvxQRmW2+4VFeOzTeTCsnPYk7K4socKqZVrS6kjn0+4wxdRNTMq5Zq0hEZk173xCHWvv42MJs/uo6j8I8ys000H8GLAKqgNPAj6Za0RhzjzFmtzFmd3t7+ww3JyLT1T8UoL6lFwBPTip3ry3h44vmqTNiDJjRYYvW2tYz3xtj/jfwnxdZ93HgcYDq6mo7k+2JyKVZa/ngVC+vHx5vplWSk4YjMZ4MNdOKGTMKdGNMgbX29MTdLwD7Lra+iARXz+AIWw+0cqJrELcrhVvK1UwrFl0y0I0xzwI3AfOMMc3Ad4GbjDFVgAUagP8WxBpF5CL8I6M8s6sRa2HD0jwqFmSq/0qMms5RLl++wOJfBKEWEbkMff4RMhyJOBLjWV+WR2GWQ9MrMU57SUQizOiYZeexTp58s4HjHQMALMnPUJiLermIRJKWHj9bD7TS0TdEWX6GmmnJORToIhFi57FOdh7rJD05gdurClmUmx7qkiTMKNBFIkR6cgIVhU7WLZ6nI1jkghToImHKPzLKm0c6yM1IptKdRcUCJxULnJd+osQsBbpIGDrW3s/2+jb6hwJ8rDQn1OVIhFCgi4SRweEArx1sp76lj3npSdxW6SHf6Qh1WRIhFOgiYaSjb5jDbf2sWZTDtSXZxMfpBCGZPgW6SIjUtdRRW1/Lsc5TZCUt4pvXforK/Eq+sa6U9GT9asrl04lFIiFQ11LHv7y1heNtAfq6V3H4VDI/fPNR6lrqFOYyYwp0kRB4tu4P9Pcso7vHQ5pjhKrSPualZlJbXxvq0iSCaSggMsf8I6O8fSQRZ7ITz/xusjMHMQYSrZOmnqZQlycRTIEuMkd6/SNkTjTTWl4EIxwiJyNz8vEefw8epyeEFUqk05SLSJAFRsd462gHT73ZwLH2fgA2Xftp+gMdeH1exuwYXp8Xr99LTVlNiKuVSKZAFwmi0z0+fr2ribePdXF1XvrkNT0r8yvZvGYzrhQXzb3NuFJcbF6zmcr8yhBXLJFMUy4iQbLjaCdvHx9vpvX5lQsonZd2zuOV+ZUKcJlVCnSRIMlMSaDS7WTtVfNITlAzLQk+BbrILPGPjPLG4fFmWiuKslhW6GRZoZppydxRoIvMgqPt/Ww/0MbAsJppSego0EWuwOBwgFcPtnOwpY95GcncXlVIXqaaaUloKNBFrkBH3zBH2/r5+KIcqtVMS0JMgS5ymXr9IzR3+SgvzMSTk8rdaqYlYUI/hSLTZK2lrrmHN450ALAwNw1HYrzCXMKGfhJFpsE7MMzWA62c9PrwZKeyYWmeruspYUeBLnIJ/pFRfr2rCWPglvI8lhVmYozmyiX8KNBFptDjG8GZMt5M65PleRRkpWh6RcKafjolpp25alBTTxMep4eashrKcyvYdbyLdxq8fHZFAQtz01mclxHqUkUuSYEuMauupY4tO7bgcrhwZ7rx+rz886uPsdL1VRxxuSwtyJxspiUSCRToErNq62txOVy4UlwA+Ac99HgTeNdfx7/c9lVKzmumJRLu1D5XYlZTTxNOx4e9VpISAyzICZDm3KMwl4ikEbrErML0Yuqb48nNSCY3a4CcTB9xiV5cKe5QlyYyIxqhS0w60tZHon89rd0JdPsGdNUgiQoaoUtMGRgK8MrBNg639rMop4gby/6CV0/8gaaeZjxOD5tWbtJFJyRiKdAlpnQNDHO8fYC1V83jmmIX8XHF3LRoZajLEpkVCnSJej2+EZq9gywrdFKUnco31pWSphOEJArpp1qilrWW95t7ePNIB8bAotx0HInxCnOJWvrJlqjUNTDMtv2tnOz2UTIvlU+UqZmWRD8FukQd/8goz+5qIs4YPrksj/ICNdOS2KBAl6jRMziCM3W8mdZfLMujwJmi6RWJKZc8Dt0Y84Qxps0Ys++sZdnGmK3GmMMTt67glikytcDoGG8c7uCptxo42t4PwFXzMxTmEnOmc2LRU8Ct5y17CHjZWrsYeHnivsicO9nt41c7G3mnoYulBRksyFIzLYldlxzCWGtfN8aUnLf4c8BNE98/DbwK/OMs1iVySW8d6WBXQxcZjkRqVi2gOEf9VyS2zfRv0jxr7WkAa+1pY8z8qVY0xtwD3APg8XhmuDmRD1lrMcaQlZrEiqIs1i6aR1KCuliIBP23wFr7uLW22lpbnZubG+zNSRTzj4zywr4W3m/uAaC8MJObl8xXmItMmOkIvdUYUzAxOi8A2mazKJHzHW7tY3t9G/6RMbLTkkJdjkhYmmmgPw98DXh44vYPs1aRyFn6hwK8Ut/GkbZ+5mcm84VVeczPcIS6LJGwdMlAN8Y8y/gO0HnGmGbgu4wH+W+NMZuAJmBjMIuU2OUdGKaxc4DrF89jlcdFXJxOEBKZynSOcvnyFA+tn+VaJIadfbHmvNRSrsv7FF+orJ5sppWapGPKRS5FvyUScmcu1pyV7CJ5bAnvHU1mx6GXcGcncK27SmEuMk06PEBCrra+ltS4+XR0LeZUh4vczDjKPZ38vyP/EerSRCKKhj4Scse9zfR2VRNnoDjfiyvdhyWNpp6mUJcmElEU6BIyZ5pplbrcNI01UuBMITFhDIBuXw8ep05EE7kcmnKROTcyOsafDrdPNtOqKashEHeS/pFOXaxZ5ApohC5zqtk7yLb9rXgHR6hY4GRBVgqOxEo2r9k8eZSLLtYsMjMKdJkzbx7pYNfxLpwpiXxxlRtPTurkY5X5lQpwkSukQJegO9NMKzstiVXFLtYszFH/FZEgUKBL0PiGR3ntUBt5mQ5WelwsLchkaUGoqxKJXgp0mXXWWg619vPqwTaGAmPkpCeHuiSRmKBAl1nVPxTg5QOtHGsfIN/pYMPSPHIzFOgic0GBLpPO7qficXqoKau57B2V3oFhTnQNcsPV81hZpGZaInNJe6YE+LCfitfnxZ3pxuvzsmXHFupa6i753J7BEfadHL/oRFF2KpvWLeSa4myFucgc0whdgPF+Ki6HC1eKC2Dytra+dspR+tiY5b0T3ew42kF8XBxXzU/HkRhPSlL8nNUtIh9SoAsATT1NuDPd5yxzOpxT9lPp6B9i2/5WTvf4WZibxifK5uNIVJCLhJICXQDwOD14fd7JkTlAj//C/VT8I6P833dOEB9n+NTyfJbkZWCMpldEQk1z6AJATVkNXr8Xr887ZT8V78AwAI7EeG6tyOera4opy89UmIuECQW6AOOn3m9esxlXiovm3mZcKS42r9lMZX4lI6NjvH6onad3jDfTAliUm64LT4iEGf1GyqQL9VM50TXItgOtdA+OUOkeb6YlIuFJgS5TeuNwB+80dJGVmsgd17gpyk699JNEJGQU6PIRZ5ppzctI4ppiF2sW5ZAYr9k5kXCnQJdJg8MBXjvYTr5zvJlWWX4mZfmhrkpEpkuBLlhrOdjax6sH2xkOjKn3ikiEUqDHuD7/CNvr2zjWPkCB08GG8jzmqTuiSERSoMe47sERmr0+brg6l5VFWeq/IhLBFOgxqHtwmBNdPpa7nRRlp/KNtaXqvyISBRToMWS8mZaXt450khAfx+I8NdMSiSYK9BjR3jfE1v2ttPaqmZZItFKgxwD/yCi/3X2ChDjDZyoLWDw/Xf1XRKKQAj2KeQeGcaUl4UiM51MV+RQ4UzS9IhLFdPpfFBoOjPHaec20FuamK8xFopxG6FGmqXO8mVaPb4QVRU7cLjXTEokVCvQo8qfD7exu8OJKTWRjtRu3S820RGKJAj0KnGmmlZuRTHWJi9UL1UxLJBYp0CPY4HCAVyeaaa1SMy2RmKdAj0DWWupbxptpjYyOkZep3isiokCPOL3+EbYfaON4xwCFWQ42LM0jR820RAQFeliqa6mjtr6Wpp4mPE4PNWU1k5eG6/WNcLLbx01LclnhVjMtEfmQ9pyFmbqWOrbs2ILX58Wd6cbr8/KD1/8Xv39/NwBuVyqb1pWy0uNSmIvIOa4o0I0xDcaYPxtj9hpjds9WUbGstr4Wl8OFK8WFIY5hfxHergp+vWc3/pFRAPVgEZELmo0pl5uttR2z8DoCNPU04c50MziUwIlWF4NDieRmDmIc7yrIReSiNIceZjxODx0DPZxqKSMuzlJa0IWNP40rpTDUpYlImLvSOXQLvGSM2WOMuedCKxhj7jHG7DbG7G5vb7/CzUW3roFhaspq6B3uxJXVwNVFp7Hxp/H6vdSU1YS6PBEJc8ZaO/MnG1NorT1ljJkPbAXut9a+PtX61dXVdvduTbWfbzgwxptHO3j/RDe3VRYyOHZsyqNcRCT2GGP2WGurL7XeFU25WGtPTdy2GWOeA64Dpgx0+ajGzgG2HWijzz/CCncWRdkpJCdUKsBF5LLNONCNMWlAnLW2b+L7TwLfn7XKYsDrh9rZ0+glOy2JjdVFLMhSZ0QRmbkrGaHnAc9NXPkmAfi1tfaFWakqyp1pppWX6eC60mw+VppNgpppicgVmnGgW2uPAStmsZaoNzAU4JWDbRRmpbDK42JJfgZLyAh1WSISJXTY4hyw1rL/dC+vH+ogMDpGgVNTKyIy+xToQdbjG2F7fSsNHYMsyEphQ3ke2WlJoS5LRKKQAv0iLtYka7r6/COc6vZzc9l8VridTOxzEBGZddoTN4ULNcnasmMLdS11l3xu18Aw75/oBj5splVVlKUwF5Gg0gh9Cmc3yQImb2vra6ccpY+OWfY0etl5rJOkhDiW5GfgSIxXDxYRmRMK9CmcaZJ1NqfDSVNP0wXXb+v189L+Vtr7hlicl87NS+YryEVkTinQp+BxevD6vJMjc4Aefw8ep+cj6/pHRvn3Pc0kxhs+u6KAq+brUEQRmXuaQ59CTVkNXr8Xr8/LmB3D6/N+pElWZ/8QMN6f/NPLC/jqmhKFuYiEjAJ9CpX5lWxesxlXiovm3mZcKS42r9lMZX4lQ4FRXqlv45c7GjnS1g9A6bw0TbGISEhpyuUiKvM/2iSroWOAbQda6R8KsNKThSc7NUTViYicS4F+GV471M67jV5y0pP4y+VFFKqZloiEEQX6JZzpF2+MocDp4GOl2VynZloiEoYU6BfRPxTglfrxZlrXFLu4Oi+Dq/O001NEwpMC/QKstXxwqpfXD7czOmpxuzS1IiLhT4F+nh7fCNv2t9LUNcgCVwq3LM3DpWZaIhIBFOjn6R8K0NLr5xNl86lUMy0RiSAKdMZPEDrh9VFVlMWCrBQ2rSvVMeUiEnFiOtBHxyzvNHSx63gXyQlxlKmZlohEsJgN9NaJZlodfUMsyc/gpiW5CnIRiWgxGej+kVF+t6eq4iYwAAAF6ElEQVSZpPg4bq8qZFFueqhLEhG5YjEV6B39Q+SkJeFIjOczywvIdzo0KheRqBETpzsOBUbZXt/K/9nRyNH2AQBK1ExLRKJM1I/Qj3cM8PJEM61VxS410xKRqBXVgf7qwTbea+omJz2JOyuLKHDqjE8RiV5hH+h1LXXU1tfS1NOEx+mhpqxmymt6wrnNtAqzUkhKiOO6EjXTEpHoF9YpV9dSx5YdW/D6vLgz3Xh9Xrbs2EJdS90F1+/zj/D8+6d4t8kLwNV5GXx80TyFuYjEhLBOutr6WlwOF64UF3EmDleKC5fDRW197TnrWWv5c3MPv9zRyImuQeLjwvptiYgERVhPuTT1NOHOdJ+zzOlw0tTTNHm/Z3CErQdaOdE1iNuVwi3leWSlqpmWiMSesA50j9OD1+fFleKaXNbj78Hj9Eze7x8O0NbnZ8PSPCoWZKqZlojErLCem6gpq8Hr9+L1eRmzY3h9Xrx+L5/wfI73JubJzzTTWq7OiCIS48I60CvzK9m8ZjOuFBfNvc04HS7+wn0fe4+ns+t4F/6RUQCSE3SCkIhIWE+5wHioV+ZX0tLjZ+v+Ftq6hynLT+dGNdMSETlH2Ac6jDfT+v27zSQnqJmWiMhUIiLQHYnx3FZZQF6mmmmJiEwlIgIdoDgnLdQliIiEtbDeKSoiItOnQBcRiRIKdBGRKHFFgW6MudUYc9AYc8QY89BsFSUiIpdvxoFujIkHHgM+BZQDXzbGlM9WYSIicnmuZIR+HXDEWnvMWjsM/Ab43OyUJSIil+tKAn0BcOKs+80Ty0REJASuJNAv1AnLfmQlY+4xxuw2xuxub2+/gs2JiMjFXMmJRc1A0Vn33cCp81ey1j4OPA5gjGk3xjTOcHvzgI4ZPjdS6T3HBr3n2HAl77l4OiuZM9fgvFzGmATgELAeOAm8A/yVtfaDGb3gpbe321pbHYzXDld6z7FB7zk2zMV7nvEI3VobMMbcB7wIxANPBCvMRUTk0q6ol4u19r+A/5qlWkRE5ApE0pmij4e6gBDQe44Nes+xIejvecZz6CIiEl4iaYQuIiIXERGBHms9Y4wxRcaYV4wxB4wxHxhj/i7UNc0FY0y8MeY9Y8x/hrqWuWCMyTLG/M4YUz/xb70m1DUFmzHm2xM/0/uMMc8aYxyhrmm2GWOeMMa0GWP2nbUs2xiz1RhzeOLWFYxth32gx2jPmADwD9bapcBq4G9j4D0D/B1wINRFzKEfAy9Ya8uAFUT5ezfGLAAeAKqttRWMHx33pdBWFRRPAbeet+wh4GVr7WLg5Yn7sy7sA50Y7BljrT1trX134vs+xn/Ro7qtgjHGDXwG+LdQ1zIXjDGZwA3ALwCstcPW2u7QVjUnEoCUifNYUrnAyYiRzlr7OtB13uLPAU9PfP808PlgbDsSAj2me8YYY0qAlcDboa0k6P4VeBAYC3Uhc2Qh0A48OTHN9G/GmKi+zqK19iSwBWgCTgM91tqXQlvVnMmz1p6G8QEbMD8YG4mEQJ9Wz5hoZIxJB34P/L21tjfU9QSLMeY2oM1auyfUtcyhBGAV8DNr7UpggCD9GR4uJuaNPweUAoVAmjHmrtBWFV0iIdCn1TMm2hhjEhkP82estbWhrifI1gK3G2MaGJ9S+4Qx5lehLSnomoFma+2Zv7x+x3jAR7MNwHFrbbu1dgSoBT4e4prmSqsxpgBg4rYtGBuJhEB/B1hsjCk1xiQxvhPl+RDXFFTGGMP43OoBa+0joa4n2Ky1/8Na67bWljD+77vdWhvVIzdrbQtwwhizZGLRemB/CEuaC03AamNM6sTP+HqifEfwWZ4Hvjbx/deAPwRjI1d06v9ciNGeMWuBrwB/NsbsnVj2PydaLUj0uB94ZmKgcgy4O8T1BJW19m1jzO+Adxk/kus9ovCMUWPMs8BNwDxjTDPwXeBh4LfGmE2M/8e2MSjb1pmiIiLRIRKmXEREZBoU6CIiUUKBLiISJRToIiJRQoEuIhIlFOgiIlFCgS4iEiUU6CIiUeL/A1CfH74/Q2RWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "    if torch.cuda.is_available():\n",
    "        predicted = model(Variable(torch.from_numpy(x_train).cuda())).cpu().data.numpy()\n",
    "    else:\n",
    "        predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "    print(predicted)\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(x_train, y_train, 'go', label='True data', alpha=0.5)\n",
    "plt.plot(x_train, predicted, '--', label='Predictions', alpha=0.5)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch实现一个简单的神经网络\n",
    "Ref:\n",
    "https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "10\n",
      "torch.Size([6, 1, 5, 5])\n",
      "tensor([[-0.0016, -0.0701,  0.0672,  0.1103,  0.0444, -0.0310,  0.0073,  0.0326,\n",
      "         -0.1183,  0.0337]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.6612, grad_fn=<MseLossBackward>)\n",
      "<MseLossBackward object at 0x000001D7D910BA20>\n",
      "<AddmmBackward object at 0x000001D7D910BA20>\n",
      "<AccumulateGrad object at 0x000001D7CDEA7978>\n",
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0047,  0.0096, -0.0064,  0.0018,  0.0012, -0.0068])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "\n",
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight\n",
    "\n",
    "\n",
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)\n",
    "\n",
    "\n",
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))\n",
    "\n",
    "\n",
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)\n",
    "\n",
    "\n",
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU\n",
    "\n",
    "\n",
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpu",
   "language": "python",
   "name": "tensorflow_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
