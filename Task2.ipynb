{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task2\n",
    "\n",
    "Ref:\n",
    "1. https://pytorch.apachecn.org/docs/1.0/pytorch_with_examples.html\n",
    "2. https://towardsdatascience.com/linear-regression-with-pytorch-eb6dedead817"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy和PyTorch实现梯度下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 34466856.06126369\n",
      "1 34041796.651330344\n",
      "2 45211583.578740835\n",
      "3 59626099.12069431\n",
      "4 61258370.17195523\n",
      "5 40139572.71317263\n",
      "6 15824319.682214037\n",
      "7 4814272.720302796\n",
      "8 1945695.618979135\n",
      "9 1223948.2239240266\n",
      "10 954681.9197147046\n",
      "11 795183.8197733068\n",
      "12 676686.6477846421\n",
      "13 582071.5300420966\n",
      "14 504583.9035260956\n",
      "15 440401.69840936945\n",
      "16 386634.17912817205\n",
      "17 341332.7607270223\n",
      "18 302939.8518256209\n",
      "19 270126.18840866745\n",
      "20 241883.55486841168\n",
      "21 217434.2241160766\n",
      "22 196165.9873330586\n",
      "23 177581.99614658038\n",
      "24 161249.44979636298\n",
      "25 146836.77054637423\n",
      "26 134065.5501333881\n",
      "27 122721.5470506805\n",
      "28 112605.95370933719\n",
      "29 103556.53131582795\n",
      "30 95418.71436941114\n",
      "31 88084.44085821051\n",
      "32 81459.91920744057\n",
      "33 75465.03973286322\n",
      "34 70019.01366020087\n",
      "35 65061.72472470868\n",
      "36 60540.29190787142\n",
      "37 56407.9421812035\n",
      "38 52624.97134725201\n",
      "39 49151.17669862496\n",
      "40 45955.02272183241\n",
      "41 43013.043808463204\n",
      "42 40301.49900454146\n",
      "43 37797.908570724634\n",
      "44 35479.74053992976\n",
      "45 33330.93133004868\n",
      "46 31336.181992073005\n",
      "47 29482.47594888112\n",
      "48 27758.61310488025\n",
      "49 26152.931044040582\n",
      "50 24655.841604171517\n",
      "51 23260.591200759394\n",
      "52 21957.40002862622\n",
      "53 20738.267424018377\n",
      "54 19596.85366296475\n",
      "55 18527.999863791825\n",
      "56 17525.745239460335\n",
      "57 16585.162599771644\n",
      "58 15701.929360777618\n",
      "59 14872.133907067322\n",
      "60 14091.589159224783\n",
      "61 13357.222654116205\n",
      "62 12665.581479667782\n",
      "63 12014.025041673232\n",
      "64 11400.075834937941\n",
      "65 10821.001377627897\n",
      "66 10274.622575428126\n",
      "67 9758.675347414108\n",
      "68 9271.139017487581\n",
      "69 8810.707949882111\n",
      "70 8375.013717422367\n",
      "71 7962.940461999127\n",
      "72 7573.293569466163\n",
      "73 7204.720128242163\n",
      "74 6855.702189248912\n",
      "75 6524.930182564732\n",
      "76 6211.539452539757\n",
      "77 5914.472072274447\n",
      "78 5632.823409770919\n",
      "79 5365.591097205994\n",
      "80 5111.9535636804485\n",
      "81 4871.471600515162\n",
      "82 4642.990574545054\n",
      "83 4426.029837015197\n",
      "84 4219.918903250808\n",
      "85 4023.9858195082907\n",
      "86 3837.7331590561225\n",
      "87 3660.68986327108\n",
      "88 3492.3194439726767\n",
      "89 3332.2195841810108\n",
      "90 3179.7966898665045\n",
      "91 3034.7426635696493\n",
      "92 2896.7259151441135\n",
      "93 2765.319012624001\n",
      "94 2640.2128910813653\n",
      "95 2521.068288829085\n",
      "96 2407.6054842131402\n",
      "97 2299.528053040138\n",
      "98 2196.599285702843\n",
      "99 2098.5568170383112\n",
      "100 2005.0659039961265\n",
      "101 1915.9409905944467\n",
      "102 1830.9386138287973\n",
      "103 1749.8910905187856\n",
      "104 1672.5826611197322\n",
      "105 1598.8282157057747\n",
      "106 1528.4732822288606\n",
      "107 1461.3337835707039\n",
      "108 1397.2728357561477\n",
      "109 1336.1188903571992\n",
      "110 1277.7632454655136\n",
      "111 1222.0355242799792\n",
      "112 1168.831006369834\n",
      "113 1118.0259666629624\n",
      "114 1069.5170632815043\n",
      "115 1023.2382862736868\n",
      "116 979.0229798498551\n",
      "117 936.7813179044499\n",
      "118 896.4116086615692\n",
      "119 857.841993903443\n",
      "120 820.9998269783212\n",
      "121 785.7694176496493\n",
      "122 752.0943986600495\n",
      "123 719.9327961827139\n",
      "124 689.2074596584393\n",
      "125 659.8234503064307\n",
      "126 631.7281310148921\n",
      "127 604.8606731222937\n",
      "128 579.1715444065276\n",
      "129 554.6090468988581\n",
      "130 531.1210973197262\n",
      "131 508.66664755667136\n",
      "132 487.2368498295916\n",
      "133 466.74475609175306\n",
      "134 447.12972129859077\n",
      "135 428.3534977680757\n",
      "136 410.3881695413717\n",
      "137 393.1950639828128\n",
      "138 376.7366629000005\n",
      "139 360.98624661949003\n",
      "140 345.90840075741767\n",
      "141 331.47122266215104\n",
      "142 317.65503422991617\n",
      "143 304.4300276716996\n",
      "144 291.7582163202109\n",
      "145 279.6295266284577\n",
      "146 268.0127520861228\n",
      "147 256.8882892718116\n",
      "148 246.24198045723583\n",
      "149 236.04019745762264\n",
      "150 226.26890200267755\n",
      "151 216.91144113204967\n",
      "152 207.9499299469171\n",
      "153 199.36329298104533\n",
      "154 191.13833412351232\n",
      "155 183.26100243249698\n",
      "156 175.71337922122308\n",
      "157 168.47854766388122\n",
      "158 161.54842730237561\n",
      "159 154.90857828186304\n",
      "160 148.54469893474354\n",
      "161 142.44822990191625\n",
      "162 136.6110948386651\n",
      "163 131.01434201046754\n",
      "164 125.65441950992118\n",
      "165 120.5148075650191\n",
      "166 115.59011747342451\n",
      "167 110.8698603456512\n",
      "168 106.34403805445584\n",
      "169 102.00716907272536\n",
      "170 97.8494546101635\n",
      "171 93.86307246113776\n",
      "172 90.04134780388182\n",
      "173 86.37792806710674\n",
      "174 82.86539334751804\n",
      "175 79.49714234185959\n",
      "176 76.26828317643654\n",
      "177 73.17238615607735\n",
      "178 70.20463385598472\n",
      "179 67.35862182820406\n",
      "180 64.62941446050498\n",
      "181 62.01200380778508\n",
      "182 59.50168878000305\n",
      "183 57.09362414784772\n",
      "184 54.78541293560847\n",
      "185 52.57197397454192\n",
      "186 50.44765374361194\n",
      "187 48.410223431013605\n",
      "188 46.45632934296271\n",
      "189 44.58215072980852\n",
      "190 42.78559530845518\n",
      "191 41.061105602599966\n",
      "192 39.40670648712276\n",
      "193 37.8199521932279\n",
      "194 36.297938594281234\n",
      "195 34.83807669757589\n",
      "196 33.437817729031586\n",
      "197 32.09352971987106\n",
      "198 30.80378938521477\n",
      "199 29.566705405407134\n",
      "200 28.379793616807827\n",
      "201 27.240878257212145\n",
      "202 26.14868852000211\n",
      "203 25.10012086735935\n",
      "204 24.09396991819075\n",
      "205 23.128931219778693\n",
      "206 22.202379941863022\n",
      "207 21.31363775042408\n",
      "208 20.46099442774473\n",
      "209 19.64237021337881\n",
      "210 18.857109390231866\n",
      "211 18.10318291984368\n",
      "212 17.379704652632604\n",
      "213 16.68550674986754\n",
      "214 16.019481434386243\n",
      "215 15.380011312500713\n",
      "216 14.766110940617589\n",
      "217 14.177064878844952\n",
      "218 13.611791276110441\n",
      "219 13.069297739634012\n",
      "220 12.548643105040513\n",
      "221 12.04877222849727\n",
      "222 11.568962189159395\n",
      "223 11.10828912803754\n",
      "224 10.66620685288989\n",
      "225 10.241839077320769\n",
      "226 9.834499186741184\n",
      "227 9.443532738166272\n",
      "228 9.068097139621408\n",
      "229 8.707853237355529\n",
      "230 8.362094288855026\n",
      "231 8.029994999239912\n",
      "232 7.71120944445163\n",
      "233 7.4052126775365315\n",
      "234 7.111481110026255\n",
      "235 6.829377392879047\n",
      "236 6.558547104570451\n",
      "237 6.298510080527083\n",
      "238 6.0488996991239485\n",
      "239 5.809261146683285\n",
      "240 5.579136043773888\n",
      "241 5.358238704108593\n",
      "242 5.146154377765665\n",
      "243 4.94253421175989\n",
      "244 4.746994591946858\n",
      "245 4.5592100544721\n",
      "246 4.378902304325023\n",
      "247 4.20580188676556\n",
      "248 4.039574672701324\n",
      "249 3.879994950175702\n",
      "250 3.726745605550346\n",
      "251 3.579565844716588\n",
      "252 3.4382030600938167\n",
      "253 3.3025203841954616\n",
      "254 3.1722017675088754\n",
      "255 3.047086280764985\n",
      "256 2.926858387508968\n",
      "257 2.81142323650201\n",
      "258 2.700599762985319\n",
      "259 2.594145162109232\n",
      "260 2.491926043693624\n",
      "261 2.393754749797914\n",
      "262 2.2994756275362773\n",
      "263 2.2089384803796324\n",
      "264 2.121967893810451\n",
      "265 2.038442530637789\n",
      "266 1.958199957123174\n",
      "267 1.8811527895540436\n",
      "268 1.8071644207972766\n",
      "269 1.7360695086369882\n",
      "270 1.6678018672012058\n",
      "271 1.602229168791235\n",
      "272 1.5392482110327208\n",
      "273 1.4787648438075776\n",
      "274 1.4206742221566846\n",
      "275 1.3648557375178365\n",
      "276 1.3112457256829027\n",
      "277 1.2597726744441238\n",
      "278 1.2103111299573337\n",
      "279 1.1627924748424754\n",
      "280 1.1171626257657197\n",
      "281 1.0733504751002145\n",
      "282 1.0312547229237086\n",
      "283 0.9908051820922377\n",
      "284 0.9519505215810669\n",
      "285 0.91462992019068\n",
      "286 0.8787792615347377\n",
      "287 0.844353118892611\n",
      "288 0.8112765519760912\n",
      "289 0.7795007363578061\n",
      "290 0.7489769923615097\n",
      "291 0.7196560413604999\n",
      "292 0.6914853218619186\n",
      "293 0.6644187703867819\n",
      "294 0.6384207792377374\n",
      "295 0.6134458515839865\n",
      "296 0.5894569154513538\n",
      "297 0.5664031931772805\n",
      "298 0.5442582046114605\n",
      "299 0.5229768643748585\n",
      "300 0.5025372382283663\n",
      "301 0.48290707487507306\n",
      "302 0.46403663697954173\n",
      "303 0.44590742951444\n",
      "304 0.42848761979547983\n",
      "305 0.411755569986669\n",
      "306 0.3956806896942695\n",
      "307 0.3802343341142306\n",
      "308 0.36539692595294326\n",
      "309 0.3511369591185101\n",
      "310 0.33743981079166674\n",
      "311 0.3242780348427987\n",
      "312 0.3116271889748206\n",
      "313 0.29947450118109387\n",
      "314 0.2877971348931194\n",
      "315 0.27658274673827665\n",
      "316 0.26580377788312826\n",
      "317 0.25544300172732476\n",
      "318 0.24549043567558596\n",
      "319 0.23592862405286427\n",
      "320 0.2267396673901136\n",
      "321 0.21790910467501978\n",
      "322 0.20942559973626917\n",
      "323 0.2012736240276057\n",
      "324 0.19344011836091585\n",
      "325 0.1859123576239771\n",
      "326 0.1786772834517047\n",
      "327 0.1717253308391038\n",
      "328 0.16504486461331772\n",
      "329 0.1586279558065164\n",
      "330 0.1524611319029438\n",
      "331 0.14653318593283843\n",
      "332 0.14083783271797073\n",
      "333 0.13536430699870924\n",
      "334 0.1301037091041416\n",
      "335 0.1250480538928727\n",
      "336 0.12018983891709403\n",
      "337 0.11552165332560171\n",
      "338 0.11103558341577496\n",
      "339 0.10672386442180551\n",
      "340 0.1025796866019237\n",
      "341 0.09859681518740962\n",
      "342 0.09476968224712473\n",
      "343 0.09109301210454449\n",
      "344 0.08755924847651292\n",
      "345 0.08416207896809161\n",
      "346 0.08089738653821443\n",
      "347 0.0777595175045687\n",
      "348 0.07474434461484825\n",
      "349 0.0718464113933982\n",
      "350 0.06906118524714375\n",
      "351 0.0663847031662705\n",
      "352 0.06381260297042282\n",
      "353 0.06133993690969304\n",
      "354 0.05896313672825797\n",
      "355 0.05667855148933634\n",
      "356 0.05448298370144531\n",
      "357 0.05237352104597165\n",
      "358 0.050345360568737604\n",
      "359 0.04839621536268174\n",
      "360 0.04652230012249328\n",
      "361 0.04472173184300701\n",
      "362 0.04299142793561381\n",
      "363 0.04132762499764172\n",
      "364 0.03972833188282363\n",
      "365 0.0381913718894383\n",
      "366 0.0367144223938917\n",
      "367 0.035294735412910894\n",
      "368 0.033929574517345386\n",
      "369 0.03261749852859691\n",
      "370 0.03135666405693443\n",
      "371 0.03014475019364633\n",
      "372 0.028979573320586108\n",
      "373 0.027859546684096653\n",
      "374 0.026783185751059903\n",
      "375 0.025748934988705778\n",
      "376 0.024754471690220724\n",
      "377 0.023798120692578827\n",
      "378 0.022879074863879304\n",
      "379 0.02199562504306908\n",
      "380 0.021146634178849544\n",
      "381 0.02033023301427174\n",
      "382 0.01954545639551776\n",
      "383 0.018791183752598133\n",
      "384 0.018065920318037717\n",
      "385 0.01736907968316727\n",
      "386 0.016698908324055454\n",
      "387 0.016054637149943304\n",
      "388 0.015435316167412562\n",
      "389 0.014840134269913612\n",
      "390 0.014267998858567391\n",
      "391 0.013717882148868123\n",
      "392 0.013188945185405008\n",
      "393 0.012680447917418998\n",
      "394 0.012191786475952718\n",
      "395 0.011722072144530695\n",
      "396 0.011270427236654168\n",
      "397 0.010836193474110863\n",
      "398 0.010418723825853614\n",
      "399 0.010017531364578617\n",
      "400 0.009631669336385734\n",
      "401 0.009260769990830245\n",
      "402 0.00890417938727417\n",
      "403 0.008561358577265445\n",
      "404 0.008231818467715792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405 0.007914890218154036\n",
      "406 0.0076102422991717236\n",
      "407 0.007317399088261361\n",
      "408 0.007035944156981442\n",
      "409 0.006765223869489671\n",
      "410 0.006504988375515627\n",
      "411 0.006254731117298641\n",
      "412 0.0060141437656883725\n",
      "413 0.0057828859638638305\n",
      "414 0.005560483899725395\n",
      "415 0.00534669246159771\n",
      "416 0.0051411880669139575\n",
      "417 0.004943573125107466\n",
      "418 0.004753590250606053\n",
      "419 0.004570908148563027\n",
      "420 0.0043952317531386394\n",
      "421 0.004226329991866311\n",
      "422 0.004063963005743782\n",
      "423 0.003907868221814133\n",
      "424 0.003757774235791646\n",
      "425 0.0036134445826605643\n",
      "426 0.0034746815990879054\n",
      "427 0.0033413322600934327\n",
      "428 0.00321302876178799\n",
      "429 0.0030896600143023854\n",
      "430 0.002971040635079384\n",
      "431 0.0028570397492194773\n",
      "432 0.0027474032372198426\n",
      "433 0.0026419655843067428\n",
      "434 0.002540570330364162\n",
      "435 0.0024430928275846215\n",
      "436 0.0023493927880103207\n",
      "437 0.002259307533598534\n",
      "438 0.0021726310398136888\n",
      "439 0.0020893002240302875\n",
      "440 0.002009184039002371\n",
      "441 0.0019321641820564217\n",
      "442 0.0018580938503871337\n",
      "443 0.0017868679566942873\n",
      "444 0.0017183584952870869\n",
      "445 0.001652504797555302\n",
      "446 0.0015891863639406166\n",
      "447 0.001528283939928253\n",
      "448 0.001469721799386801\n",
      "449 0.0014134147446973465\n",
      "450 0.0013592692482690012\n",
      "451 0.0013072141211023138\n",
      "452 0.0012571473275251254\n",
      "453 0.0012089973830075647\n",
      "454 0.0011627071254036178\n",
      "455 0.0011181939519529938\n",
      "456 0.0010753774812485408\n",
      "457 0.0010342053496561557\n",
      "458 0.000994611683865871\n",
      "459 0.0009565455377665874\n",
      "460 0.0009199503966200491\n",
      "461 0.000884744154862698\n",
      "462 0.0008508868252166128\n",
      "463 0.000818332298989026\n",
      "464 0.0007870293320846389\n",
      "465 0.0007569231554554784\n",
      "466 0.0007279701051291654\n",
      "467 0.0007001300346415988\n",
      "468 0.0006733558136799364\n",
      "469 0.000647609118767136\n",
      "470 0.0006228465847963261\n",
      "471 0.000599030558283486\n",
      "472 0.0005761275360913948\n",
      "473 0.000554106774383443\n",
      "474 0.0005329292184454014\n",
      "475 0.0005125585147576427\n",
      "476 0.0004929678955308888\n",
      "477 0.00047413238811123406\n",
      "478 0.00045602213558336847\n",
      "479 0.0004386039787349561\n",
      "480 0.0004218463887963744\n",
      "481 0.000405729909746083\n",
      "482 0.00039023264300212747\n",
      "483 0.00037533236531581295\n",
      "484 0.00036099822160785686\n",
      "485 0.0003472114812733772\n",
      "486 0.000333951759818445\n",
      "487 0.0003212036485617766\n",
      "488 0.00030894384856003257\n",
      "489 0.0002971501571586111\n",
      "490 0.0002858060142218789\n",
      "491 0.0002748971208710797\n",
      "492 0.00026440707553774905\n",
      "493 0.0002543170449783879\n",
      "494 0.0002446125635912076\n",
      "495 0.00023527883773989103\n",
      "496 0.00022630172457571325\n",
      "497 0.00021767189410018222\n",
      "498 0.00020936966643606057\n",
      "499 0.00020138254768556468\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# N是批大小；D_in是输入维度\n",
    "# H是隐藏层维度；D_out是输出维度  \n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 产生随机输入和输出数据\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# 随机初始化权重\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 前向传播：计算预测值y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # 计算并显示loss（损失）\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # 反向传播，计算w1、w2对loss的梯度\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # 更新权重\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 28370104.0\n",
      "1 23409750.0\n",
      "2 22715178.0\n",
      "3 22987066.0\n",
      "4 21953032.0\n",
      "5 18679472.0\n",
      "6 13839488.0\n",
      "7 9091042.0\n",
      "8 5550368.0\n",
      "9 3329876.5\n",
      "10 2065934.0\n",
      "11 1366360.75\n",
      "12 973140.6875\n",
      "13 739970.6875\n",
      "14 591678.375\n",
      "15 489760.875\n",
      "16 414740.875\n",
      "17 356522.75\n",
      "18 309582.0625\n",
      "19 270740.75\n",
      "20 238073.734375\n",
      "21 210277.546875\n",
      "22 186418.734375\n",
      "23 165779.328125\n",
      "24 147823.875\n",
      "25 132141.296875\n",
      "26 118396.5\n",
      "27 106299.921875\n",
      "28 95630.34375\n",
      "29 86192.375\n",
      "30 77830.015625\n",
      "31 70388.9375\n",
      "32 63748.9609375\n",
      "33 57818.6875\n",
      "34 52508.203125\n",
      "35 47739.046875\n",
      "36 43452.04296875\n",
      "37 39592.51171875\n",
      "38 36121.0\n",
      "39 32984.53125\n",
      "40 30152.61328125\n",
      "41 27588.12890625\n",
      "42 25263.12890625\n",
      "43 23152.515625\n",
      "44 21234.26171875\n",
      "45 19488.654296875\n",
      "46 17899.015625\n",
      "47 16450.130859375\n",
      "48 15127.771484375\n",
      "49 13921.2880859375\n",
      "50 12818.2578125\n",
      "51 11809.248046875\n",
      "52 10885.666015625\n",
      "53 10039.494140625\n",
      "54 9263.59765625\n",
      "55 8551.916015625\n",
      "56 7898.40234375\n",
      "57 7298.33154296875\n",
      "58 6746.59423828125\n",
      "59 6239.1337890625\n",
      "60 5772.2548828125\n",
      "61 5342.517578125\n",
      "62 4946.626953125\n",
      "63 4581.7314453125\n",
      "64 4245.2216796875\n",
      "65 3934.80908203125\n",
      "66 3648.36328125\n",
      "67 3383.8857421875\n",
      "68 3139.490234375\n",
      "69 2913.63916015625\n",
      "70 2704.99609375\n",
      "71 2511.98291015625\n",
      "72 2333.3427734375\n",
      "73 2168.016845703125\n",
      "74 2014.947021484375\n",
      "75 1873.186279296875\n",
      "76 1741.832275390625\n",
      "77 1620.1011962890625\n",
      "78 1507.231201171875\n",
      "79 1402.566162109375\n",
      "80 1305.4993896484375\n",
      "81 1215.4127197265625\n",
      "82 1131.8065185546875\n",
      "83 1054.170166015625\n",
      "84 982.0662231445312\n",
      "85 915.1198120117188\n",
      "86 852.90673828125\n",
      "87 795.076171875\n",
      "88 741.3018798828125\n",
      "89 691.3077392578125\n",
      "90 644.8032836914062\n",
      "91 601.5411376953125\n",
      "92 561.2828979492188\n",
      "93 523.81982421875\n",
      "94 488.9407958984375\n",
      "95 456.57135009765625\n",
      "96 426.4933166503906\n",
      "97 398.47235107421875\n",
      "98 372.3677978515625\n",
      "99 348.06201171875\n",
      "100 325.40765380859375\n",
      "101 304.27862548828125\n",
      "102 284.5705871582031\n",
      "103 266.18731689453125\n",
      "104 249.03868103027344\n",
      "105 233.02859497070312\n",
      "106 218.08023071289062\n",
      "107 204.12649536132812\n",
      "108 191.09426879882812\n",
      "109 178.9214630126953\n",
      "110 167.55026245117188\n",
      "111 156.925537109375\n",
      "112 146.99945068359375\n",
      "113 137.72018432617188\n",
      "114 129.044921875\n",
      "115 120.93278503417969\n",
      "116 113.34774780273438\n",
      "117 106.25421142578125\n",
      "118 99.616455078125\n",
      "119 93.40653991699219\n",
      "120 87.59636688232422\n",
      "121 82.1580810546875\n",
      "122 77.06631469726562\n",
      "123 72.30007934570312\n",
      "124 67.83833312988281\n",
      "125 63.659751892089844\n",
      "126 59.74592971801758\n",
      "127 56.079978942871094\n",
      "128 52.64626693725586\n",
      "129 49.42679214477539\n",
      "130 46.41082000732422\n",
      "131 43.58329391479492\n",
      "132 40.93341064453125\n",
      "133 38.4481201171875\n",
      "134 36.118099212646484\n",
      "135 33.93408966064453\n",
      "136 31.88504409790039\n",
      "137 29.962366104125977\n",
      "138 28.159446716308594\n",
      "139 26.467628479003906\n",
      "140 24.880290985107422\n",
      "141 23.390094757080078\n",
      "142 21.991703033447266\n",
      "143 20.678754806518555\n",
      "144 19.446502685546875\n",
      "145 18.28976821899414\n",
      "146 17.203166961669922\n",
      "147 16.182764053344727\n",
      "148 15.224031448364258\n",
      "149 14.323747634887695\n",
      "150 13.478046417236328\n",
      "151 12.683164596557617\n",
      "152 11.936836242675781\n",
      "153 11.234979629516602\n",
      "154 10.575496673583984\n",
      "155 9.95544719696045\n",
      "156 9.372843742370605\n",
      "157 8.824844360351562\n",
      "158 8.309867858886719\n",
      "159 7.825782775878906\n",
      "160 7.370542526245117\n",
      "161 6.941555023193359\n",
      "162 6.538741111755371\n",
      "163 6.159611701965332\n",
      "164 5.803279876708984\n",
      "165 5.46787691116333\n",
      "166 5.152318000793457\n",
      "167 4.855137825012207\n",
      "168 4.575348854064941\n",
      "169 4.312346935272217\n",
      "170 4.064535140991211\n",
      "171 3.8313214778900146\n",
      "172 3.6117472648620605\n",
      "173 3.4051036834716797\n",
      "174 3.2106239795684814\n",
      "175 3.0271928310394287\n",
      "176 2.854617118835449\n",
      "177 2.6922051906585693\n",
      "178 2.5389480590820312\n",
      "179 2.3944671154022217\n",
      "180 2.2586801052093506\n",
      "181 2.130612373352051\n",
      "182 2.0100388526916504\n",
      "183 1.8963308334350586\n",
      "184 1.7891223430633545\n",
      "185 1.6880745887756348\n",
      "186 1.5929591655731201\n",
      "187 1.503248929977417\n",
      "188 1.418637752532959\n",
      "189 1.3389068841934204\n",
      "190 1.2636964321136475\n",
      "191 1.1928093433380127\n",
      "192 1.1259827613830566\n",
      "193 1.0630104541778564\n",
      "194 1.0034773349761963\n",
      "195 0.9475091695785522\n",
      "196 0.894588053226471\n",
      "197 0.8447014093399048\n",
      "198 0.7976616024971008\n",
      "199 0.7531919479370117\n",
      "200 0.7114170789718628\n",
      "201 0.6718798279762268\n",
      "202 0.6345902681350708\n",
      "203 0.5993919372558594\n",
      "204 0.5662325620651245\n",
      "205 0.5348590612411499\n",
      "206 0.5053324699401855\n",
      "207 0.47735095024108887\n",
      "208 0.4510590732097626\n",
      "209 0.42616569995880127\n",
      "210 0.40272846817970276\n",
      "211 0.38053539395332336\n",
      "212 0.35963550209999084\n",
      "213 0.33983170986175537\n",
      "214 0.32115739583969116\n",
      "215 0.3035648763179779\n",
      "216 0.2869569659233093\n",
      "217 0.27120494842529297\n",
      "218 0.25633639097213745\n",
      "219 0.24235083162784576\n",
      "220 0.22912806272506714\n",
      "221 0.21660462021827698\n",
      "222 0.20480450987815857\n",
      "223 0.19363978505134583\n",
      "224 0.1830798089504242\n",
      "225 0.17310544848442078\n",
      "226 0.16369101405143738\n",
      "227 0.1547972708940506\n",
      "228 0.14638827741146088\n",
      "229 0.13846176862716675\n",
      "230 0.13094410300254822\n",
      "231 0.12385943531990051\n",
      "232 0.11712519824504852\n",
      "233 0.11083048582077026\n",
      "234 0.10484541207551956\n",
      "235 0.09914770722389221\n",
      "236 0.09378352761268616\n",
      "237 0.08873625099658966\n",
      "238 0.08395892381668091\n",
      "239 0.0794367641210556\n",
      "240 0.07514828443527222\n",
      "241 0.07111163437366486\n",
      "242 0.06728196144104004\n",
      "243 0.06368320435285568\n",
      "244 0.060254357755184174\n",
      "245 0.05702269449830055\n",
      "246 0.053960636258125305\n",
      "247 0.051078591495752335\n",
      "248 0.04832662269473076\n",
      "249 0.04574275761842728\n",
      "250 0.04329228401184082\n",
      "251 0.040992822498083115\n",
      "252 0.038812294602394104\n",
      "253 0.03673113137483597\n",
      "254 0.034776028245687485\n",
      "255 0.03292570263147354\n",
      "256 0.031177710741758347\n",
      "257 0.02951899543404579\n",
      "258 0.02794843167066574\n",
      "259 0.026470419019460678\n",
      "260 0.025070803239941597\n",
      "261 0.023740626871585846\n",
      "262 0.022483179345726967\n",
      "263 0.021290544420480728\n",
      "264 0.020173635333776474\n",
      "265 0.019113849848508835\n",
      "266 0.018098847940564156\n",
      "267 0.01715283840894699\n",
      "268 0.016245294362306595\n",
      "269 0.015399854630231857\n",
      "270 0.014598287642002106\n",
      "271 0.013837498612701893\n",
      "272 0.013109344057738781\n",
      "273 0.0124294962733984\n",
      "274 0.011782707646489143\n",
      "275 0.011168837547302246\n",
      "276 0.010588692501187325\n",
      "277 0.01004070695489645\n",
      "278 0.009524978697299957\n",
      "279 0.009035369381308556\n",
      "280 0.00857475958764553\n",
      "281 0.008131071925163269\n",
      "282 0.007714614272117615\n",
      "283 0.007321516051888466\n",
      "284 0.006951451301574707\n",
      "285 0.006595853250473738\n",
      "286 0.006260150112211704\n",
      "287 0.005945179611444473\n",
      "288 0.0056445240043103695\n",
      "289 0.0053595746867358685\n",
      "290 0.0050895619206130505\n",
      "291 0.004839013330638409\n",
      "292 0.004598099738359451\n",
      "293 0.00437016598880291\n",
      "294 0.004154830239713192\n",
      "295 0.003950738348066807\n",
      "296 0.0037567904219031334\n",
      "297 0.00357550336048007\n",
      "298 0.003398932982236147\n",
      "299 0.003232897724956274\n",
      "300 0.003072845283895731\n",
      "301 0.0029295505955815315\n",
      "302 0.002790839644148946\n",
      "303 0.002659804420545697\n",
      "304 0.0025336877442896366\n",
      "305 0.0024168165400624275\n",
      "306 0.00230220565572381\n",
      "307 0.0021968972869217396\n",
      "308 0.002091319765895605\n",
      "309 0.0019966920372098684\n",
      "310 0.001906863646581769\n",
      "311 0.0018212393624708056\n",
      "312 0.0017401899676769972\n",
      "313 0.0016617011278867722\n",
      "314 0.0015881925355643034\n",
      "315 0.0015149798709899187\n",
      "316 0.00144812127109617\n",
      "317 0.001387203810736537\n",
      "318 0.001326522440649569\n",
      "319 0.001268784049898386\n",
      "320 0.0012160900514572859\n",
      "321 0.0011636798735707998\n",
      "322 0.0011155640240758657\n",
      "323 0.0010668240720406175\n",
      "324 0.0010241242125630379\n",
      "325 0.0009813467040657997\n",
      "326 0.0009421269642189145\n",
      "327 0.0009038536227308214\n",
      "328 0.0008676046272739768\n",
      "329 0.0008333164150826633\n",
      "330 0.0007993818144313991\n",
      "331 0.0007668256293982267\n",
      "332 0.0007385165663436055\n",
      "333 0.0007094308384694159\n",
      "334 0.0006829488556832075\n",
      "335 0.0006573396385647357\n",
      "336 0.000632104987744242\n",
      "337 0.0006083046901039779\n",
      "338 0.0005866608116775751\n",
      "339 0.000564680143725127\n",
      "340 0.000544409267604351\n",
      "341 0.0005259248428046703\n",
      "342 0.0005061306292191148\n",
      "343 0.0004884349764324725\n",
      "344 0.00047112212632782757\n",
      "345 0.00045483047142624855\n",
      "346 0.0004395404539536685\n",
      "347 0.0004249568737577647\n",
      "348 0.0004105217522010207\n",
      "349 0.00039635272696614265\n",
      "350 0.00038319217856042087\n",
      "351 0.00037067208904772997\n",
      "352 0.000358849938493222\n",
      "353 0.0003470267402008176\n",
      "354 0.0003360879491083324\n",
      "355 0.00032583664869889617\n",
      "356 0.0003150780685245991\n",
      "357 0.0003056966233998537\n",
      "358 0.0002956432581413537\n",
      "359 0.00028617921634577215\n",
      "360 0.000277449085842818\n",
      "361 0.0002694329887162894\n",
      "362 0.0002611473319120705\n",
      "363 0.0002538388653192669\n",
      "364 0.0002464910503476858\n",
      "365 0.00023894925834611058\n",
      "366 0.00023199318093247712\n",
      "367 0.0002257110900245607\n",
      "368 0.00021938254940323532\n",
      "369 0.00021355581702664495\n",
      "370 0.00020725184003822505\n",
      "371 0.00020195916295051575\n",
      "372 0.00019598775543272495\n",
      "373 0.00019093696027994156\n",
      "374 0.0001857484458014369\n",
      "375 0.00018067480414174497\n",
      "376 0.00017557438695803285\n",
      "377 0.00017154387023765594\n",
      "378 0.0001669988560024649\n",
      "379 0.00016272917855530977\n",
      "380 0.00015840449486859143\n",
      "381 0.00015415321104228497\n",
      "382 0.00015062594320625067\n",
      "383 0.0001463553635403514\n",
      "384 0.00014311412815004587\n",
      "385 0.00013964538811706007\n",
      "386 0.00013580231461673975\n",
      "387 0.00013266368478070945\n",
      "388 0.00012934747792314738\n",
      "389 0.00012603538925759494\n",
      "390 0.0001236458629136905\n",
      "391 0.00012055862316628918\n",
      "392 0.00011804568202933297\n",
      "393 0.00011501632980071008\n",
      "394 0.0001130822638515383\n",
      "395 0.00011039826495107263\n",
      "396 0.00010776257840916514\n",
      "397 0.00010544534598011523\n",
      "398 0.00010329553333576769\n",
      "399 0.00010122256935574114\n",
      "400 9.875618707155809e-05\n",
      "401 9.670609142631292e-05\n",
      "402 9.453245729673654e-05\n",
      "403 9.269877773476765e-05\n",
      "404 9.066697384696454e-05\n",
      "405 8.85886256583035e-05\n",
      "406 8.665326458867639e-05\n",
      "407 8.512672502547503e-05\n",
      "408 8.350491407327354e-05\n",
      "409 8.186936611309648e-05\n",
      "410 8.036673534661531e-05\n",
      "411 7.88614124758169e-05\n",
      "412 7.736137922620401e-05\n",
      "413 7.585556886624545e-05\n",
      "414 7.45311554055661e-05\n",
      "415 7.314707909245044e-05\n",
      "416 7.159965025493875e-05\n",
      "417 7.026350067462772e-05\n",
      "418 6.906787166371942e-05\n",
      "419 6.783597928006202e-05\n",
      "420 6.662552186753601e-05\n",
      "421 6.529848906211555e-05\n",
      "422 6.406609463738278e-05\n",
      "423 6.294430932030082e-05\n",
      "424 6.180749187478796e-05\n",
      "425 6.085934001021087e-05\n",
      "426 5.951779894530773e-05\n",
      "427 5.8547244407236576e-05\n",
      "428 5.756253085564822e-05\n",
      "429 5.667374352924526e-05\n",
      "430 5.5673452152404934e-05\n",
      "431 5.4776573961135e-05\n",
      "432 5.373710882849991e-05\n",
      "433 5.2984814828960225e-05\n",
      "434 5.2000879804836586e-05\n",
      "435 5.142452573636547e-05\n",
      "436 5.0481488869991153e-05\n",
      "437 4.957756391377188e-05\n",
      "438 4.8875743232201785e-05\n",
      "439 4.817622175323777e-05\n",
      "440 4.7357425501104444e-05\n",
      "441 4.686367901740596e-05\n",
      "442 4.5970489736646414e-05\n",
      "443 4.5219272578833625e-05\n",
      "444 4.4564902054844424e-05\n",
      "445 4.387005537864752e-05\n",
      "446 4.3239371734671295e-05\n",
      "447 4.2540155845927075e-05\n",
      "448 4.211795385344885e-05\n",
      "449 4.126775456825271e-05\n",
      "450 4.077022822457366e-05\n",
      "451 4.007682218798436e-05\n",
      "452 3.9635211578570306e-05\n",
      "453 3.8920559745747596e-05\n",
      "454 3.853666567010805e-05\n",
      "455 3.8035650504752994e-05\n",
      "456 3.753494456759654e-05\n",
      "457 3.7132460420252755e-05\n",
      "458 3.6553268728312105e-05\n",
      "459 3.5979828680865467e-05\n",
      "460 3.562626079656184e-05\n",
      "461 3.50236332451459e-05\n",
      "462 3.455254773143679e-05\n",
      "463 3.417413608985953e-05\n",
      "464 3.3669894037302583e-05\n",
      "465 3.321164331282489e-05\n",
      "466 3.288890729891136e-05\n",
      "467 3.249975270591676e-05\n",
      "468 3.214307798771188e-05\n",
      "469 3.169593037455343e-05\n",
      "470 3.110970283159986e-05\n",
      "471 3.0845258152112365e-05\n",
      "472 3.0406476071220823e-05\n",
      "473 2.994614806084428e-05\n",
      "474 2.950711495941505e-05\n",
      "475 2.9309710953384638e-05\n",
      "476 2.88185037788935e-05\n",
      "477 2.8466198273235932e-05\n",
      "478 2.821363159455359e-05\n",
      "479 2.8079150069970638e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480 2.7727637643693015e-05\n",
      "481 2.748104634520132e-05\n",
      "482 2.7248377591604367e-05\n",
      "483 2.6756219085655175e-05\n",
      "484 2.6419060304760933e-05\n",
      "485 2.609869625302963e-05\n",
      "486 2.5703317078296095e-05\n",
      "487 2.5511279091006145e-05\n",
      "488 2.527002288843505e-05\n",
      "489 2.4927547201514244e-05\n",
      "490 2.454393688822165e-05\n",
      "491 2.427127765258774e-05\n",
      "492 2.4160628527170047e-05\n",
      "493 2.3896263883216307e-05\n",
      "494 2.3695114578003995e-05\n",
      "495 2.3450545995729044e-05\n",
      "496 2.3188546037999913e-05\n",
      "497 2.288237737957388e-05\n",
      "498 2.2840389647171833e-05\n",
      "499 2.2481308405986056e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "\n",
    "# N是批大小； D_in 是输入维度；\n",
    "# H 是隐藏层维度； D_out 是输出维度\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 产生随机输入和输出数据\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# 随机初始化权重\n",
    "w1 = torch.randn(D_in, H, device=device)\n",
    "w2 = torch.randn(H, D_out, device=device)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 前向传播：计算预测值y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # 计算并输出loss；loss是存储在PyTorch的tensor中的标量，维度是()（零维标量）；我们使用loss.item()得到tensor中的纯python数值。\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # 反向传播，计算w1、w2对loss的梯度\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # 使用梯度下降更新权重\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy和PyTorch实现线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2333)\n",
    "\n",
    "# create dummy data for training\n",
    "x_values = [i for i in range(11)]\n",
    "x_train = np.array(x_values, dtype=np.float32)\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "\n",
    "y_values = [2*i + 1 + np.random.random() for i in x_values]\n",
    "y_train = np.array(y_values, dtype=np.float32)\n",
    "y_train = y_train.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.5297776]\n",
      " [ 3.8944874]\n",
      " [ 5.242338 ]\n",
      " [ 7.9269605]\n",
      " [ 9.191861 ]\n",
      " [11.387585 ]\n",
      " [13.287302 ]\n",
      " [15.729676 ]\n",
      " [17.43938  ]\n",
      " [19.30325  ]\n",
      " [21.393335 ]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class linearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(linearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "    \n",
    "inputDim = 1        # takes variable 'x' \n",
    "outputDim = 1       # takes variable 'y'\n",
    "learningRate = 0.01 \n",
    "epochs = 100\n",
    "\n",
    "model = linearRegression(inputDim, outputDim)\n",
    "##### For GPU #######\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    \n",
    "criterion = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(304.3204, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 0, loss 304.3204040527344\n",
      "tensor(25.2250, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 1, loss 25.225019454956055\n",
      "tensor(2.4562, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 2, loss 2.4561500549316406\n",
      "tensor(0.5951, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 3, loss 0.5950658321380615\n",
      "tensor(0.4394, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 4, loss 0.43940770626068115\n",
      "tensor(0.4229, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 5, loss 0.42289915680885315\n",
      "tensor(0.4178, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 6, loss 0.41778305172920227\n",
      "tensor(0.4136, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 7, loss 0.41363832354545593\n",
      "tensor(0.4096, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 8, loss 0.40961435437202454\n",
      "tensor(0.4056, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 9, loss 0.4056415557861328\n",
      "tensor(0.4017, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 10, loss 0.40171337127685547\n",
      "tensor(0.3978, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 11, loss 0.3978293240070343\n",
      "tensor(0.3940, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 12, loss 0.39398863911628723\n",
      "tensor(0.3902, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 13, loss 0.39019060134887695\n",
      "tensor(0.3864, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 14, loss 0.386435329914093\n",
      "tensor(0.3827, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 15, loss 0.382721871137619\n",
      "tensor(0.3790, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 16, loss 0.37904980778694153\n",
      "tensor(0.3754, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 17, loss 0.3754187822341919\n",
      "tensor(0.3718, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 18, loss 0.37182819843292236\n",
      "tensor(0.3683, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 19, loss 0.3682779371738434\n",
      "tensor(0.3648, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 20, loss 0.3647671043872833\n",
      "tensor(0.3613, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 21, loss 0.36129558086395264\n",
      "tensor(0.3579, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 22, loss 0.3578629195690155\n",
      "tensor(0.3545, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 23, loss 0.35446837544441223\n",
      "tensor(0.3511, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 24, loss 0.35111185908317566\n",
      "tensor(0.3478, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 25, loss 0.3477928340435028\n",
      "tensor(0.3445, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 26, loss 0.34451085329055786\n",
      "tensor(0.3413, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 27, loss 0.34126555919647217\n",
      "tensor(0.3381, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 28, loss 0.33805641531944275\n",
      "tensor(0.3349, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 29, loss 0.3348832130432129\n",
      "tensor(0.3317, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 30, loss 0.331745445728302\n",
      "tensor(0.3286, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 31, loss 0.3286426365375519\n",
      "tensor(0.3256, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 32, loss 0.32557448744773865\n",
      "tensor(0.3225, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 33, loss 0.32254067063331604\n",
      "tensor(0.3195, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 34, loss 0.31954070925712585\n",
      "tensor(0.3166, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 35, loss 0.3165741562843323\n",
      "tensor(0.3136, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 36, loss 0.3136405646800995\n",
      "tensor(0.3107, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 37, loss 0.3107401132583618\n",
      "tensor(0.3079, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 38, loss 0.30787190794944763\n",
      "tensor(0.3050, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 39, loss 0.30503562092781067\n",
      "tensor(0.3022, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 40, loss 0.30223119258880615\n",
      "tensor(0.2995, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 41, loss 0.2994578778743744\n",
      "tensor(0.2967, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 42, loss 0.29671570658683777\n",
      "tensor(0.2940, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 43, loss 0.29400408267974854\n",
      "tensor(0.2913, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 44, loss 0.2913227379322052\n",
      "tensor(0.2887, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 45, loss 0.2886713445186615\n",
      "tensor(0.2860, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 46, loss 0.28604966402053833\n",
      "tensor(0.2835, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 47, loss 0.2834570109844208\n",
      "tensor(0.2809, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 48, loss 0.2808935344219208\n",
      "tensor(0.2784, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 49, loss 0.2783585786819458\n",
      "tensor(0.2759, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 50, loss 0.27585190534591675\n",
      "tensor(0.2734, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 51, loss 0.2733733057975769\n",
      "tensor(0.2709, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 52, loss 0.27092236280441284\n",
      "tensor(0.2685, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 53, loss 0.2684987485408783\n",
      "tensor(0.2661, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 54, loss 0.2661021649837494\n",
      "tensor(0.2637, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 55, loss 0.2637324929237366\n",
      "tensor(0.2614, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 56, loss 0.2613891661167145\n",
      "tensor(0.2591, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 57, loss 0.25907209515571594\n",
      "tensor(0.2568, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 58, loss 0.25678086280822754\n",
      "tensor(0.2545, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 59, loss 0.2545151114463806\n",
      "tensor(0.2523, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 60, loss 0.25227487087249756\n",
      "tensor(0.2501, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 61, loss 0.2500593364238739\n",
      "tensor(0.2479, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 62, loss 0.24786873161792755\n",
      "tensor(0.2457, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 63, loss 0.2457025945186615\n",
      "tensor(0.2436, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 64, loss 0.24356068670749664\n",
      "tensor(0.2414, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 65, loss 0.24144266545772552\n",
      "tensor(0.2393, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 66, loss 0.23934826254844666\n",
      "tensor(0.2373, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 67, loss 0.23727728426456451\n",
      "tensor(0.2352, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 68, loss 0.23522943258285522\n",
      "tensor(0.2332, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 69, loss 0.23320436477661133\n",
      "tensor(0.2312, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 70, loss 0.2312019020318985\n",
      "tensor(0.2292, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 71, loss 0.229221910238266\n",
      "tensor(0.2273, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 72, loss 0.22726397216320038\n",
      "tensor(0.2253, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 73, loss 0.22532792389392853\n",
      "tensor(0.2234, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 74, loss 0.22341345250606537\n",
      "tensor(0.2215, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 75, loss 0.22152024507522583\n",
      "tensor(0.2196, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 76, loss 0.21964842081069946\n",
      "tensor(0.2178, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 77, loss 0.21779736876487732\n",
      "tensor(0.2160, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 78, loss 0.21596702933311462\n",
      "tensor(0.2142, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 79, loss 0.21415705978870392\n",
      "tensor(0.2124, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 80, loss 0.21236738562583923\n",
      "tensor(0.2106, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 81, loss 0.21059773862361908\n",
      "tensor(0.2088, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 82, loss 0.20884771645069122\n",
      "tensor(0.2071, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 83, loss 0.2071174532175064\n",
      "tensor(0.2054, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 84, loss 0.2054062932729721\n",
      "tensor(0.2037, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 85, loss 0.20371432602405548\n",
      "tensor(0.2020, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 86, loss 0.20204132795333862\n",
      "tensor(0.2004, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 87, loss 0.20038677752017975\n",
      "tensor(0.1988, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 88, loss 0.19875091314315796\n",
      "tensor(0.1971, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 89, loss 0.19713331758975983\n",
      "tensor(0.1955, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 90, loss 0.19553370773792267\n",
      "tensor(0.1940, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 91, loss 0.19395194947719574\n",
      "tensor(0.1924, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 92, loss 0.19238781929016113\n",
      "tensor(0.1908, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 93, loss 0.1908412128686905\n",
      "tensor(0.1893, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 94, loss 0.18931184709072113\n",
      "tensor(0.1878, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 95, loss 0.18779969215393066\n",
      "tensor(0.1863, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 96, loss 0.18630437552928925\n",
      "tensor(0.1848, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 97, loss 0.18482564389705658\n",
      "tensor(0.1834, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 98, loss 0.1833634376525879\n",
      "tensor(0.1819, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 99, loss 0.18191766738891602\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # Converting inputs and labels to Variable\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = Variable(torch.from_numpy(x_train).cuda())\n",
    "        labels = Variable(torch.from_numpy(y_train).cuda())\n",
    "    else:\n",
    "        inputs = Variable(torch.from_numpy(x_train))\n",
    "        labels = Variable(torch.from_numpy(y_train))\n",
    "\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = criterion(outputs, labels)\n",
    "    print(loss)\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.9438095]\n",
      " [ 3.0146153]\n",
      " [ 5.085421 ]\n",
      " [ 7.156227 ]\n",
      " [ 9.227033 ]\n",
      " [11.297838 ]\n",
      " [13.368645 ]\n",
      " [15.43945  ]\n",
      " [17.510256 ]\n",
      " [19.581062 ]\n",
      " [21.651867 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "    if torch.cuda.is_available():\n",
    "        predicted = model(Variable(torch.from_numpy(x_train).cuda())).cpu().data.numpy()\n",
    "    else:\n",
    "        predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "    print(predicted)\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(x_train, y_train, 'go', label='True data', alpha=0.5)\n",
    "plt.plot(x_train, predicted, '--', label='Predictions', alpha=0.5)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch实现一个简单的神经网络\n",
    "Ref:\n",
    "https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "10\n",
      "torch.Size([6, 1, 5, 5])\n",
      "tensor([[-0.0016, -0.0701,  0.0672,  0.1103,  0.0444, -0.0310,  0.0073,  0.0326,\n",
      "         -0.1183,  0.0337]], grad_fn=<AddmmBackward>)\n",
      "tensor(0.6612, grad_fn=<MseLossBackward>)\n",
      "<MseLossBackward object at 0x000001D7D910BA20>\n",
      "<AddmmBackward object at 0x000001D7D910BA20>\n",
      "<AccumulateGrad object at 0x000001D7CDEA7978>\n",
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0047,  0.0096, -0.0064,  0.0018,  0.0012, -0.0068])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "\n",
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight\n",
    "\n",
    "\n",
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)\n",
    "\n",
    "\n",
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))\n",
    "\n",
    "\n",
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)\n",
    "\n",
    "\n",
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU\n",
    "\n",
    "\n",
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpu",
   "language": "python",
   "name": "tensorflow_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
