{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch的基本概念\n",
    "\n",
    "### What is PyTorch? Why?\n",
    "\n",
    "It's a Python-based scientific computing package targeted at two sets of audiences:\n",
    "\n",
    "- a replacement for NumPy to use the power of GPUs\n",
    "- a deep learning research platform that provides maximum flexibility and speed\n",
    "\n",
    "Here are some (a lot) of the strengths of PyTorch,\n",
    "\n",
    "> - Strong and very active development community. Many of the lead developers are at Facebook AI but it is not really a Facebook project. It is very open. It is used internally at Facebook for research, with Caffe2 being used for production.\n",
    "> - Easy to learn. If you are moderately skilled with python and numpy you will be able to get started quickly.\n",
    "> - It is becoming tightly aligned with Caffe 2 to meet production requirements.\n",
    "> - It has ONNX support for porting models to other frameworks. This is a big plus. It's not limited for use together with Caffe2.\n",
    "> - Fast.ai has switched from Keras/TensorFlow to PyTorch. If you are not familiar with Fast.ai you should follow the link and check them out. They are doing great stuff.\n",
    "> - Dynamic execution graphs. This is a big one, and it's why PyTorch has a nice \"feel\". You can execute your model graphs as you development them. This is the influence from Chainer. This is the \"Define-by-Run\" feature. It's a large part of what makes PyTorch fast and easy to use.\n",
    "> - Easy debugging. This is largely a result of the item above. It is more like plan old python debugging. Not too bad.\n",
    "> - PyTorch tensors are essentially equivalent to numpy arrays. You can switch back and forth with ease and they use the same memory space.\n",
    "> - Strong GPU acceleration. NVIDIA CUDA is well utilized and it is very simple to load and execute code on the GPU.\n",
    "> - It has TensorBoard support. tensorboard-pytorch\n",
    "> - It has multi-processing and distributed computing modules for multi-GPU multi-node communication and execution.\n",
    "\n",
    "Ref:\n",
    "1. https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py\n",
    "2. https://www.pugetsystems.com/labs/hpc/Why-You-Should-Consider-PyTorch-includes-Install-and-a-few-examples-1193/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 安装PyTorch\n",
    "\n",
    "Windows使用Anaconda环境安装:\n",
    "- Python 3.6\n",
    "- CUDA 10.0\n",
    "- PyTorch 1.1.0\n",
    "\n",
    "```bash\n",
    "# 安装PyTorch\n",
    "conda install pytorch torchvision cudatoolkit=10.0 -c pytorch\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:  1.1.0\n",
      "CUDA available:  True\n",
      "CUDA version:  10.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"PyTorch version: \", torch.__version__ )\n",
    "print(\"CUDA available: \", torch.cuda.is_available())\n",
    "print(\"CUDA version: \", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5.432945966720581 seconds \n",
      " norm =  1000061.8\n"
     ]
    }
   ],
   "source": [
    "n = 10000\n",
    "\n",
    "A = np.random.randn(n,n).astype('float32')\n",
    "B = np.random.randn(n,n).astype('float32')\n",
    "\n",
    "start_time = time.time()\n",
    "nrm = np.linalg.norm(A@B)\n",
    "print(' {} seconds '.format(time.time() - start_time))\n",
    "print(' norm = ',nrm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5.8335113525390625 seconds \n",
      " norm =  tensor(973085.1875)\n"
     ]
    }
   ],
   "source": [
    "tA = torch.randn(n,n)\n",
    "tB = torch.randn(n,n)\n",
    "\n",
    "start_time = time.time()\n",
    "tnrm = (tA@tB).norm()\n",
    "print(' {} seconds '.format(time.time() - start_time))\n",
    "print(' norm = ',tnrm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch with CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.2640421390533447 seconds \n",
      " norm =  tensor(1000077.6250, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "gA = torch.randn(n,n, device=\"cuda\")\n",
    "gB = torch.randn(n,n, device=\"cuda\")\n",
    "\n",
    "start_time = time.time()\n",
    "gnrm = (gA@gB).norm()\n",
    "print(' {} seconds '.format(time.time() - start_time))\n",
    "print(' norm = ',gnrm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch基础概念及通用代码实现流程\n",
    "\n",
    "Ref: \n",
    "1. https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
    "2. https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
    "3. https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e\n",
    "4. https://www.jiqizhixin.com/articles/0225"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpu",
   "language": "python",
   "name": "tensorflow_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
